{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":330722,"status":"ok","timestamp":1663953219011,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"1gy3jEKQqn_d","outputId":"bf3a9b1f-539b-4b62-9f98-0817cd37a689"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting talib-binary\n","  Downloading talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from talib-binary) (1.21.6)\n","Installing collected packages: talib-binary\n","Successfully installed talib-binary-0.4.19\n","Downloading...\n","From: https://drive.google.com/uc?id=17OYeD6dlfwX28FNoUpqXkLVpzjUtTCRW\n","To: /content/ThaiDepression.zip\n","100% 8.78M/8.78M [00:00<00:00, 49.0MB/s]\n","Archive:  /content/ThaiDepression.zip\n","   creating: ThaiDepression/2016/\n","  inflating: ThaiDepression/2016/ahb-2016-10.csv  \n","  inflating: ThaiDepression/2016/ahb-2016-10.pdf  \n","  inflating: ThaiDepression/2016/ahb-2016-11.csv  \n","  inflating: ThaiDepression/2016/ahb-2016-11.pdf  \n","  inflating: ThaiDepression/2016/ahb-2016-12.csv  \n","  inflating: ThaiDepression/2016/ahb-2016-12.pdf  \n","   creating: ThaiDepression/2017/\n","  inflating: ThaiDepression/2017/ahb-2017-01.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-01.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-02.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-02.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-03.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-03.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-04.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-04.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-05.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-05.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-06.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-06.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-07.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-07.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-08.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-08.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-09.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-09.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-10.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-10.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-11.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-11.pdf  \n","  inflating: ThaiDepression/2017/ahb-2017-12.csv  \n","  inflating: ThaiDepression/2017/ahb-2017-12.pdf  \n","   creating: ThaiDepression/2018/\n","  inflating: ThaiDepression/2018/ahb-2018-01.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-01.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-02.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-02.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-03.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-03.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-04.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-04.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-05.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-05.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-06.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-06.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-07.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-07.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-08.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-08.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-09.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-09.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-10.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-10.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-11.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-11.pdf  \n","  inflating: ThaiDepression/2018/ahb-2018-12.csv  \n","  inflating: ThaiDepression/2018/ahb-2018-12.pdf  \n","   creating: ThaiDepression/2019/\n","  inflating: ThaiDepression/2019/ahb-2019-01.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-01.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-02.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-02.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-03.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-03.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-04.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-04.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-05.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-05.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-06.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-06.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-07.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-07.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-08.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-08.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-09.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-09.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-10.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-10.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-11.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-11.pdf  \n","  inflating: ThaiDepression/2019/ahb-2019-12.csv  \n","  inflating: ThaiDepression/2019/ahb-2019-12.pdf  \n","   creating: ThaiDepression/2020/\n","  inflating: ThaiDepression/2020/ahb-2020-01.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-01.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-02.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-02.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-03.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-03.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-04.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-04.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-05.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-05.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-06.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-06.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-07.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-07.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-08.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-08.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-09.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-09.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-10.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-10.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-11.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-11.pdf  \n","  inflating: ThaiDepression/2020/ahb-2020-12.csv  \n","  inflating: ThaiDepression/2020/ahb-2020-12.pdf  \n","   creating: ThaiDepression/2021/\n","  inflating: ThaiDepression/2021/ahb-2021-01.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-01.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-02.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-02.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-03.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-03.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-04.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-04.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-05.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-05.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-06.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-06.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-07.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-07.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-08.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-08.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-09.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-09.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-10.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-10.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-11.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-11.pdf  \n","  inflating: ThaiDepression/2021/ahb-2021-12.csv  \n","  inflating: ThaiDepression/2021/ahb-2021-12.pdf  \n","   creating: ThaiDepression/2022/\n","  inflating: ThaiDepression/2022/ahb-2022-01.csv  \n","  inflating: ThaiDepression/2022/ahb-2022-01.pdf  \n","  inflating: ThaiDepression/2022/ahb-2022-02.csv  \n","  inflating: ThaiDepression/2022/ahb-2022-02.pdf  \n","  inflating: ThaiDepression/2022/ahb-2022-03.csv  \n","  inflating: ThaiDepression/2022/ahb-2022-03.pdf  \n","  inflating: ThaiDepression/2022/ahb-2022-04.csv  \n","  inflating: ThaiDepression/2022/ahb-2022-04.pdf  \n","  inflating: ThaiDepression/2022/ahb-2022-05.csv  \n","  inflating: ThaiDepression/2022/ahb-2022-05.pdf  \n","  inflating: ThaiDepression/2022/ahb-2022-06.csv  \n","  inflating: ThaiDepression/2022/ahb-2022-06.pdf  \n","  inflating: ThaiDepression/2022/ahb-2022-07.csv  \n","  inflating: ThaiDepression/2022/ahb-2022-07.pdf  \n","\u001b[K     |████████████████████████████████| 22.5 MB 115.7 MB/s \n","\u001b[K     |████████████████████████████████| 211 kB 46.4 MB/s \n","\u001b[K     |████████████████████████████████| 1.8 MB 47.3 MB/s \n","\u001b[K     |████████████████████████████████| 97 kB 5.8 MB/s \n","\u001b[K     |████████████████████████████████| 298 kB 72.1 MB/s \n","\u001b[K     |████████████████████████████████| 11.2 MB 42.7 MB/s \n","\u001b[K     |████████████████████████████████| 9.9 MB 8.5 MB/s \n","\u001b[K     |████████████████████████████████| 959 kB 73.1 MB/s \n","\u001b[K     |████████████████████████████████| 9.8 MB 63.6 MB/s \n","\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.2.5)\n","Collecting pandas\n","  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n","\u001b[K     |████████████████████████████████| 11.3 MB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Installing collected packages: pandas\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.2.5\n","    Uninstalling pandas-1.2.5:\n","      Successfully uninstalled pandas-1.2.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","greykite 0.4.0 requires pandas<1.3,>=1.1.3, but you have pandas 1.3.5 which is incompatible.\u001b[0m\n","Successfully installed pandas-1.3.5\n","Found existing installation: matplotlib 3.5.3\n","Uninstalling matplotlib-3.5.3:\n","  Would remove:\n","    /usr/local/lib/python3.7/dist-packages/matplotlib-3.5.3-py3.7-nspkg.pth\n","    /usr/local/lib/python3.7/dist-packages/matplotlib-3.5.3.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/matplotlib/*\n","    /usr/local/lib/python3.7/dist-packages/mpl_toolkits/axes_grid/*\n","    /usr/local/lib/python3.7/dist-packages/mpl_toolkits/axes_grid1/*\n","    /usr/local/lib/python3.7/dist-packages/mpl_toolkits/axisartist/*\n","    /usr/local/lib/python3.7/dist-packages/mpl_toolkits/mplot3d/*\n","    /usr/local/lib/python3.7/dist-packages/mpl_toolkits/tests/*\n","    /usr/local/lib/python3.7/dist-packages/pylab.py\n","Proceed (y/n)? y\n","  Successfully uninstalled matplotlib-3.5.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting matplotlib==3.1.3\n","  Downloading matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n","\u001b[K     |████████████████████████████████| 13.1 MB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.21.6)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.4.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.3) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n","Installing collected packages: matplotlib\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","greykite 0.4.0 requires matplotlib>=3.4.1, but you have matplotlib 3.1.3 which is incompatible.\n","greykite 0.4.0 requires pandas<1.3,>=1.1.3, but you have pandas 1.3.5 which is incompatible.\u001b[0m\n","Successfully installed matplotlib-3.1.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tsai\n","  Downloading tsai-0.3.1-py3-none-any.whl (241 kB)\n","\u001b[K     |████████████████████████████████| 241 kB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: nbformat>=5.1.3 in /usr/local/lib/python3.7/dist-packages (from tsai) (5.4.0)\n","Requirement already satisfied: psutil>=5.4.8 in /usr/local/lib/python3.7/dist-packages (from tsai) (5.4.8)\n","Collecting torch<1.12,>=1.7.0\n","  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n","\u001b[K     |████████████████████████████████| 750.6 MB 9.6 kB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tsai) (21.3)\n","Requirement already satisfied: fastai>=2.5.6 in /usr/local/lib/python3.7/dist-packages (from tsai) (2.7.9)\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from tsai) (21.1.3)\n","Requirement already satisfied: imbalanced-learn>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tsai) (0.8.1)\n","Collecting pyts>=0.12.0\n","  Downloading pyts-0.12.0-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 57.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (2.23.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (6.0)\n","Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (7.1.2)\n","Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (0.0.7)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (1.7.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (1.3.5)\n","Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (3.4.1)\n","Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (0.13.1+cu113)\n","Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (1.0.3)\n","Requirement already satisfied: fastcore<1.6,>=1.4.5 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (1.5.25)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.6->tsai) (3.1.3)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn>=0.8.0->tsai) (1.21.6)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn>=0.8.0->tsai) (1.1.0)\n","Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.3->tsai) (5.1.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.3->tsai) (2.16.1)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.3->tsai) (4.11.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.3->tsai) (4.3.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai) (22.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai) (4.1.1)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai) (0.18.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai) (4.12.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai) (5.9.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=5.1.3->tsai) (3.8.1)\n","Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.7/dist-packages (from pyts>=0.12.0->tsai) (0.56.2)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts>=0.12.0->tsai) (0.39.1)\n","Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts>=0.12.0->tsai) (57.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.5.6->tsai) (3.1.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (1.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (2.11.3)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (8.1.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (1.0.8)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (1.0.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (3.0.10)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (0.6.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (2.0.6)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (2.4.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (0.10.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (3.3.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (2.0.8)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (0.4.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (4.64.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.6->tsai) (3.0.7)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tsai) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4->fastai>=2.5.6->tsai) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.5.6->tsai) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.5.6->tsai) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.5.6->tsai) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.5.6->tsai) (3.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.5.6->tsai) (0.7.8)\n","Collecting torchvision>=0.8.2\n","  Downloading torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[K     |████████████████████████████████| 19.1 MB 30.8 MB/s \n","\u001b[?25h  Downloading torchvision-0.13.0-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[K     |████████████████████████████████| 19.1 MB 29.7 MB/s \n","\u001b[?25h  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[K     |████████████████████████████████| 21.0 MB 23.0 MB/s \n","\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.5.6->tsai) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4->fastai>=2.5.6->tsai) (2.0.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.5.6->tsai) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.5.6->tsai) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.5.6->tsai) (1.4.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai>=2.5.6->tsai) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai>=2.5.6->tsai) (2022.2.1)\n","Installing collected packages: torch, torchvision, pyts, tsai\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.13.1+cu113\n","    Uninstalling torchvision-0.13.1+cu113:\n","      Successfully uninstalled torchvision-0.13.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\n","Successfully installed pyts-0.12.0 torch-1.11.0 torchvision-0.12.0 tsai-0.3.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sktime\n","  Downloading sktime-0.13.2-py3-none-any.whl (6.9 MB)\n","\u001b[K     |████████████████████████████████| 6.9 MB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: pandas<1.5.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.3.5)\n","Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.7/dist-packages (from sktime) (0.56.2)\n","Requirement already satisfied: numpy<1.23,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.21.6)\n","Requirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from sktime) (0.13.2)\n","Requirement already satisfied: scikit-learn<1.2.0,>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.0.2)\n","Collecting deprecated>=1.2.13\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: scipy<1.9.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.7.3)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.13->sktime) (1.14.1)\n","Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime) (57.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime) (4.12.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime) (0.39.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.5.0,>=1.1.0->sktime) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.5.0,>=1.1.0->sktime) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<1.5.0,>=1.1.0->sktime) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.2.0,>=0.24.0->sktime) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.2.0,>=0.24.0->sktime) (3.1.0)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime) (21.3)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime) (0.5.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->statsmodels>=0.12.1->sktime) (3.0.9)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.53->sktime) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.53->sktime) (3.8.1)\n","Installing collected packages: deprecated, sktime\n","Successfully installed deprecated-1.2.13 sktime-0.13.2\n","Downloading...\n","From: https://drive.google.com/uc?id=1h6z8hj_4cXAZ-qtssjWIEga6zcDyfP1b\n","To: /content/AHB_13.csv\n","100% 2.10k/2.10k [00:00<00:00, 1.43MB/s]\n"]}],"source":["! pip install talib-binary\n","! gdown 17OYeD6dlfwX28FNoUpqXkLVpzjUtTCRW\n","! unzip /content/ThaiDepression.zip\n","! pip install -qqq greykite\n","! pip install --upgrade pandas\n","! python -m pip uninstall matplotlib\n","! pip install matplotlib==3.1.3\n","! pip install tsai\n","! pip install sktime\n","! gdown 1h6z8hj_4cXAZ-qtssjWIEga6zcDyfP1b"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7280,"status":"ok","timestamp":1663953226278,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"8IVrhjIssYij","outputId":"fd3555a4-1f1b-406d-c2ca-9c0a7530a55e"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu is available.\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import talib\n","from greykite.framework.templates.autogen.forecast_config import ForecastConfig\n","from greykite.framework.templates.autogen.forecast_config import MetadataParam\n","from greykite.framework.templates.forecaster import Forecaster \n","from greykite.framework.templates.model_templates import ModelTemplateEnum\n","from greykite.framework.utils.result_summary import summarize_grid_search_results\n","import plotly\n","import warnings\n","from statsmodels.tsa.stattools import adfuller\n","from statsmodels.graphics.tsaplots import  plot_pacf\n","from statsmodels.tsa.stattools import pacf\n","import itertools\n","from statsmodels.tsa.arima.model import ARIMA\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_absolute_percentage_error\n","from sklearn.model_selection import train_test_split\n","from tsai.all import *\n","from tsai.inference import load_learner\n","from pandas import Series\n","from sklearn.preprocessing import MinMaxScaler\n","from pmdarima import arima\n","from sktime.forecasting.arima import AutoARIMA\n","from sktime.forecasting.model_selection import ExpandingWindowSplitter\n","from sktime.forecasting.model_evaluation import evaluate\n","from sktime.utils.plotting import plot_series\n","import seaborn as sns\n","from sklearn import preprocessing\n","from IPython.display import display\n","from fastai.imports import *\n","from sklearn import metrics\n","from pandas.tseries.offsets import DateOffset\n","import math\n","from sklearn.impute import SimpleImputer\n","import torch\n","from sklearn.model_selection import TimeSeriesSplit\n","import json\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.optim as optim                       \n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"{device}\" \" is available.\")"]},{"cell_type":"markdown","metadata":{"id":"RPuV1-8SvJqN"},"source":["#Data Preparation"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":20061,"status":"ok","timestamp":1663953246336,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"mmbJEFk9rNc6"},"outputs":[],"source":["path = \"/content/ThaiDepression/\"\n","year_list = os.listdir(path)\n","year_list.sort()\n","\n","df_list = []\n","name_list = []\n"," \n","for i in range(len(year_list)):\n","    file_list = os.listdir(path+year_list[i])\n","    file_list.sort()\n","    for f in file_list:\n","        if '.csv' in f:\n","            # print(f)\n","            name_list.append(f[4:11])\n","            df_list.append(pd.read_csv(path+year_list[i]+'/'+f, header=None, index_col=False))\n","\n","\n","finished_process = []\n","for i in range(len(df_list[0])):\n","    finished_process.append(pd.DataFrame(df_list[0].iloc[i,:]).T.reset_index(drop=True))\n","    finished_process[i]['y-m'] = name_list[0]\n","\n","\n","i=1\n","while(i != len(df_list)):\n","    # print('i=' + str(i))\n","    if len(df_list[i]) == 89:\n","        insert = { 0 : np.nan, 1 : np.nan, 2 : np.nan, 3 : np.nan, 4 : np.nan, 5 : np.nan, 6 :np.nan, 7 : np.nan, 8 : np.nan, 9 : np.nan, 'y-m' : name_list[i]}\n","        finished_process[89] = pd.concat([finished_process[89], pd.DataFrame(insert, index=[0])], ignore_index = True)\n","    for j in range(len(df_list[i])):\n","        # print('j=' + str(j))\n","        if len(df_list[i].columns) == 6 :\n","            insert = { 0 : df_list[i].iloc[j,0], 1 : df_list[i].iloc[j,1], 2 : np.nan, 3 : df_list[i].iloc[j,2], 4 : df_list[i].iloc[j,3], 5 : np.nan, 6 :np.nan, 7 : df_list[i].iloc[j,4], 8 : np.nan, 9 : df_list[i].iloc[j,5], 'y-m' : name_list[i]}\n","            finished_process[j] = pd.concat([finished_process[j], pd.DataFrame(insert, index=[0])], ignore_index = True)\n","        else:\n","            finished_process[j] = pd.concat([finished_process[j], pd.DataFrame(df_list[i].iloc[j,:]).T.reset_index(drop=True)], ignore_index = True)\n","            finished_process[j]['y-m'][i] = name_list[i]\n","    i=i+1\n","\n","\n","for i in range(len(finished_process)):\n","    finished_process[i].index = finished_process[i]['y-m'].values\n","    finished_process[i].drop(['y-m'], axis=1, inplace=True)\n","    # finished_process[i].index = pd.to_datetime(finished_process[i].index, format='%Y-%m')\n","    finished_process[i] = finished_process[i].replace(',','', regex=True)  \n","    finished_process[i] = finished_process[i].apply(pd.to_numeric)\n","\n","    # pct_change(1) = (value on this index - value on previous index) / value on previous index\n","\n","    finished_process[i]['0_pct_change'] = finished_process[i][0].pct_change(1)\n","    finished_process[i]['1_pct_change'] = finished_process[i][1].pct_change(1)\n","    finished_process[i]['2_pct_change'] = finished_process[i][2].pct_change(1)\n","    finished_process[i]['3_pct_change'] = finished_process[i][3].pct_change(1)\n","    finished_process[i]['4_pct_change'] = finished_process[i][4].pct_change(1)\n","    finished_process[i]['5_pct_change'] = finished_process[i][5].pct_change(1)\n","    finished_process[i]['6_pct_change'] = finished_process[i][6].pct_change(1)\n","    finished_process[i]['7_pct_change'] = finished_process[i][7].pct_change(1)\n","    finished_process[i]['8_pct_change'] = finished_process[i][8].pct_change(1)\n","    finished_process[i]['9_pct_change'] = finished_process[i][9].pct_change(1)\n","    # finished_process[i].loc[finished_process[i][3] == np.nan, '3_pct_change'] = np.nan\n","    # finished_process[i]['3_pct_change'] = finished_process[i][3].apply(lambda x: np.nan if x == np.nan else x)\n","\n","\n","finished_process[89][3] = pd.read_csv('/content/AHB_13.csv', index_col=0)['Bangkok']"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1663953246337,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"7x3Q7AAqrPzB"},"outputs":[],"source":["AHB_data = []\n","AHB_list = [9, 15, 21, 30, 39, 48, 53, 61, 66, 72, 80, 88, 89]\n","province_AHB_list = [\n","            [0, 1, 2, 3, 4, 5, 6, 7, 8],\n","            [0, 10, 11, 12, 13, 14],\n","            [0, 16, 17, 18, 19, 20],\n","            [0, 22, 23, 24, 25, 26, 27, 28, 29],\n","            [0, 31, 32, 33, 34, 35, 36, 37, 38],\n","            [0, 40, 41, 42, 43, 44, 45, 46, 47],\n","            [0, 49, 50, 51, 52],\n","            [0, 54, 55, 56, 57, 58, 59, 60],\n","            [0, 62, 63, 64, 65],\n","            [0, 67, 68, 69, 70, 71],\n","            [0, 73, 74, 75, 76, 77, 78, 79],\n","            [0, 81, 82, 83, 84, 85, 86, 87],\n","            [0, 89]\n","            ]\n","# AHB\n","for j in range(len(AHB_list)):\n","    AHB = pd.DataFrame(finished_process[AHB_list[j]][3].values, columns=['AHB_'+str(j+1)])\n","    for i in range(len(province_AHB_list[j])):\n","        if i == 0:\n","            continue\n","        AHB = pd.concat([AHB, finished_process[province_AHB_list[j][i]][3].reset_index(drop='index')], axis = 1)\n","    temp = [str(x) for x in province_AHB_list[j]]\n","    temp[0] = 'AHB_'+str(j+1)\n","    AHB.columns = temp\n","    AHB['y'] = AHB['AHB_'+str(j+1)].shift(-1)\n","    AHB.index = finished_process[9][3].index\n","    AHB.drop(AHB.tail(1).index,inplace=True)\n","    AHB_data.append(AHB)\n","\n","\n","#normalized_AHB_data = []\n","#for i in range(len(AHB_data)):\n","#    temp = (AHB_data[i]-AHB_data[i].mean())/AHB_data[i].std()\n","    # temp = (AHB_data[i]-AHB_data[i].min())/(AHB_data[i].max()-AHB_data[i].min())\n","#    normalized_AHB_data.append(temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":500,"status":"ok","timestamp":1663692715755,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"Kj7J_AOmJGm1","outputId":"2701076c-a29f-4d94-9322-ef7bfa9d60e7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[          AHB_1      1     2      3     4      5     6      7      8         y\n"," 2016-10   53656  10383  5315   4266  4057  15408  2002   9011   3214   53653.0\n"," 2016-11   53653  10383  5315   4265  4057  15406  2002   9011   3214   53650.0\n"," 2016-12   53650  10382  5315   4265  4057  15404  2002   9011   3214   53978.0\n"," 2017-01   53978  10382  5655   4265  4057  15393  2002   9011   3213   53976.0\n"," 2017-02   53976  10382  5655   4264  4057  15392  2002   9011   3213   57981.0\n"," ...         ...    ...   ...    ...   ...    ...   ...    ...    ...       ...\n"," 2022-02  137049  25005  9671  10674  7947  47073  6876  17426  12377  137869.0\n"," 2022-03  137869  25223  9711  10788  7988  47292  6924  17444  12499  139544.0\n"," 2022-04  139544  25456  9781  10932  8095  48020  7004  17606  12650  140152.0\n"," 2022-05  140152  25619  9833  10977  8122  48155  7026  17703  12717  141246.0\n"," 2022-06  141246  25836  9884  11069  8161  48616  7090  17783  12807  142732.0\n"," \n"," [69 rows x 10 columns],\n","          AHB_2     10     11     12     13    14        y\n"," 2016-10  32594   2558   7875   8783   7396  5982  32701.0\n"," 2016-11  32701   2558   7895   8782   7484  5982  32737.0\n"," 2016-12  32737   2558   7895   8781   7521  5982  33170.0\n"," 2017-01  33170   2558   7892   8780   7605  6335  33397.0\n"," 2017-02  33397   2558   7946   8777   7624  6492  33434.0\n"," ...        ...    ...    ...    ...    ...   ...      ...\n"," 2022-02  65602  10127  18648  15374  12485  8968  66121.0\n"," 2022-03  66121  10165  18907  15455  12558  9036  66821.0\n"," 2022-04  66821  10318  19057  15632  12662  9152  67013.0\n"," 2022-05  67013  10320  19148  15671  12703  9171  67399.0\n"," 2022-06  67399  10323  19298  15743  12779  9256  68142.0\n"," \n"," [69 rows x 7 columns],\n","          AHB_3    16     17    18     19    20        y\n"," 2016-10  29595  3432   6997  5336  10476  3354  29694.0\n"," 2016-11  29694  3439   6997  5336  10568  3354  29696.0\n"," 2016-12  29696  3442   6996  5336  10568  3354  29796.0\n"," 2017-01  29796  3442   6993  5336  10671  3354  29809.0\n"," 2017-02  29809  3442   6993  5335  10685  3354  30518.0\n"," ...        ...   ...    ...   ...    ...   ...      ...\n"," 2022-02  67062  7786  16047  9671  24860  8698  67430.0\n"," 2022-03  67430  7826  16104  9734  25026  8740  68130.0\n"," 2022-04  68130  7886  16315  9822  25322  8785  68290.0\n"," 2022-05  68290  7911  16374  9845  25359  8801  68718.0\n"," 2022-06  68718  7954  16448  9904  25557  8855  69140.0\n"," \n"," [69 rows x 7 columns],\n","           AHB_4     22     23     24     25     26    27    28    29         y\n"," 2016-10   55602   9731   8862  10474   6518   7734  3388  5583  3312   55624.0\n"," 2016-11   55624   9745   8870  10474   6518   7734  3388  5583  3312   56376.0\n"," 2016-12   56376   9751   9748  10346   6517   7732  3388  5583  3311   57141.0\n"," 2017-01   57141   9765   9847  10916   6515   7714  3388  5686  3310   57501.0\n"," 2017-02   57501   9758  10183  10913   6513   7739  3388  5697  3310   61764.0\n"," ...         ...    ...    ...    ...    ...    ...   ...   ...   ...       ...\n"," 2022-02  115899  34524  17729  16251  11106  13864  8766  8873  4786  116651.0\n"," 2022-03  116651  34826  17831  16303  11156  13945  8854  8916  4820  117874.0\n"," 2022-04  117874  35225  18031  16414  11327  14061  8975  8973  4868  118271.0\n"," 2022-05  118271  35361  18051  16454  11373  14140  9016  8992  4884  119106.0\n"," 2022-06  119106  35606  18176  16573  11464  14230  9094  9040  4923  120106.0\n"," \n"," [69 rows x 10 columns],\n","          AHB_5     31     32     33     34     35    36    37     38        y\n"," 2016-10  43044   8067   7603   6324   5171   6132  4589  1507   3651  43691.0\n"," 2016-11  43691   8238   7603   6494   5231   6198  4633  1522   3772  43747.0\n"," 2016-12  43747   8237   7604   6519   5230   6197  4668  1522   3770  44651.0\n"," 2017-01  44651   8276   7858   6640   5305   6319  4696  1720   3837  45269.0\n"," 2017-02  45269   8276   7872   6766   5330   6622  4731  1725   3947  45893.0\n"," ...        ...    ...    ...    ...    ...    ...   ...   ...    ...      ...\n"," 2022-02  82865  14325  11368  12353  12811  10281  8750  3051   9926  83252.0\n"," 2022-03  83252  14407  11406  12419  12907  10347  8806  2945  10015  84282.0\n"," 2022-04  84282  14536  11443  12626  13047  10473  8878  3128  10151  84534.0\n"," 2022-05  84534  14583  11462  12685  13079  10489  8899  3128  10209  85008.0\n"," 2022-06  85008  14699  11508  12815  13179  10575  8947  2945  10340  86057.0\n"," \n"," [69 rows x 10 columns],\n","          AHB_6     40    41     42     43    44     45    46    47        y\n"," 2016-10  38746   6456  4271   6913   6004  2572   7908  1265  3357  38789.0\n"," 2016-11  38789   6454  4271   6911   6004  2572   7908  1312  3357  38787.0\n"," 2016-12  38787   6455  4271   6910   6002  2572   7908  1312  3357  40075.0\n"," 2017-01  40075   6456  4271   6910   6000  3356   8408  1318  3356  40108.0\n"," 2017-02  40108   6456  4270   6908   6034  3354   8406  1325  3355  40124.0\n"," ...        ...    ...   ...    ...    ...   ...    ...   ...   ...      ...\n"," 2022-02  83895  17468  6813   9854  13492  7004  18398  3466  7400  84627.0\n"," 2022-03  84627  17553  6847   9893  13608  7103  18653  3504  7466  85627.0\n"," 2022-04  85627  17705  6902   9946  13773  7231  18936  3541  7593  85935.0\n"," 2022-05  85935  17723  6917   9965  13846  7270  19009  3561  7644  86644.0\n"," 2022-06  86644  17801  6959  10008  13953  7372  19222  3591  7738  87629.0\n"," \n"," [69 rows x 10 columns],\n","           AHB_7     49     50     51     52         y\n"," 2016-10   54437   8477  19045  14203  12712   54776.0\n"," 2016-11   54776   8520  19234  14228  12794   55087.0\n"," 2016-12   55087   8532  19470  14233  12852   55661.0\n"," 2017-01   55661   8689  19816  14231  12925   56045.0\n"," 2017-02   56045   8806  20064  14231  12944   56431.0\n"," ...         ...    ...    ...    ...    ...       ...\n"," 2022-02   99183  16680  36632  22628  23243   99574.0\n"," 2022-03   99574  16722  36856  22628  23368  100517.0\n"," 2022-04  100517  16809  37331  22886  23491  100726.0\n"," 2022-05  100726  16862  37389  22928  23547  101269.0\n"," 2022-06  101269  16962  37627  23028  23652  102001.0\n"," \n"," [69 rows x 6 columns],\n","           AHB_8    54     55     56    57     58     59     60         y\n"," 2016-10   66020  6450  10454   3474  6155  16452   8279  14756   66074.0\n"," 2016-11   66074  6453  10454   3474  6155  16452   8281  14805   66392.0\n"," 2016-12   66392  6453  10454   3474  6155  16451   8575  14830   66664.0\n"," 2017-01   66664  6578  10476   3479  6156  16454   8581  14940   66809.0\n"," 2017-02   66809  6576  10473   3565  6156  16498   8581  14960   66990.0\n"," ...         ...   ...    ...    ...   ...    ...    ...    ...       ...\n"," 2022-02  101692  7599  14045  10373  8881  26313  11916  22565  102022.0\n"," 2022-03  102022  7628  14085  10383  8924  26313  11933  22756  102461.0\n"," 2022-04  102461  7663  14184  10457  8974  26313  11988  22882  103219.0\n"," 2022-05  103219  7681  14204  10491  9004  26921  11998  22920  103680.0\n"," 2022-06  103680  7711  14281  10544  9043  27021  12042  23038  104396.0\n"," \n"," [69 rows x 9 columns],\n","           AHB_9     62     63     64     65         y\n"," 2016-10   71525  11333  28200  17408  14584   72897.0\n"," 2016-11   72897  11504  29177  17617  14599   73177.0\n"," 2016-12   73177  11526  29236  17745  14670   73739.0\n"," 2017-01   73739  11553  29379  17970  14837   77859.0\n"," 2017-02   77859  14161  30229  18145  15324   80960.0\n"," ...         ...    ...    ...    ...    ...       ...\n"," 2022-02  142673  23083  61201  33359  25030  144393.0\n"," 2022-03  144393  23217  61826  33541  25809  145690.0\n"," 2022-04  145690  23427  62345  33727  26191  146809.0\n"," 2022-05  146809  23533  62562  33815  26899  148170.0\n"," 2022-06  148170  23689  63020  33994  27467  149595.0\n"," \n"," [69 rows x 6 columns],\n","          AHB_10     67     68     69     70    71         y\n"," 2016-10   46691   4919   6937  11902  20439  2494   47805.0\n"," 2016-11   47805   5881   6937  12045  20438  2504   48152.0\n"," 2016-12   48152   5890   6937  12379  20436  2510   48704.0\n"," 2017-01   48704   6015   6937  12640  20598  2514   49247.0\n"," 2017-02   49247   6015   6937  13100  20657  2538   50208.0\n"," ...         ...    ...    ...    ...    ...   ...       ...\n"," 2022-02  100893   9696  12781  26560  45225  6631  101478.0\n"," 2022-03  101478   9980  12847  26734  45225  6692  102176.0\n"," 2022-04  102176  10036  12906  26872  45617  6745  102429.0\n"," 2022-05  102429  10053  12935  26940  45725  6776  102850.0\n"," 2022-06  102850  10101  12973  27100  45861  6815  103588.0\n"," \n"," [69 rows x 7 columns],\n","          AHB_11    73     74     75    76    77    78    79        y\n"," 2016-10   39516  3775  13009  13674  2705  2870  2036  1447  39587.0\n"," 2016-11   39587  3777  13049  13684  2715  2870  2035  1457  39749.0\n"," 2016-12   39749  3778  13184  13681  2716  2870  2035  1485  40062.0\n"," 2017-01   40062  3804  13338  13681  2802  2868  2033  1536  40223.0\n"," 2017-02   40223  3816  13420  13681  2852  2868  2043  1543  40498.0\n"," ...         ...   ...    ...    ...   ...   ...   ...   ...      ...\n"," 2022-02   77436  6495  25917  26461  5663  4399  4678  3823  77972.0\n"," 2022-03   77972  6551  26018  26693  5683  4434  4758  3835  78767.0\n"," 2022-04   78767  6595  26198  26985  5744  4464  4887  3894  78995.0\n"," 2022-05   78995  6615  26268  27067  5769  4479  4896  3901  79596.0\n"," 2022-06   79596  6723  26478  27192  5823  4496  4944  3940  80413.0\n"," \n"," [69 rows x 9 columns],\n","          AHB_12     81     82     83     84    85     86    87        y\n"," 2016-10   52629   6950   9535   9569   6626  3597  14123  2229  52966.0\n"," 2016-11   52966   6959   9626   9608   6684  3598  14231  2260  53279.0\n"," 2016-12   53279   6970   9682   9683   6735  3598  14330  2281  53848.0\n"," 2017-01   53848   6998   9718   9713   6766  3738  14637  2278  54214.0\n"," 2017-02   54214   7006   9934   9726   6804  3738  14682  2324  54482.0\n"," ...         ...    ...    ...    ...    ...   ...    ...   ...      ...\n"," 2022-02   92166  11159  14522  14001  10945  7509  28774  5256  92725.0\n"," 2022-03   92725  11218  14532  14098  11015  7550  29019  5293  93639.0\n"," 2022-04   93639  11315  14652  14205  11083  7633  29390  5361  94019.0\n"," 2022-05   94019  11360  14699  14262  11126  7662  29519  5391  94776.0\n"," 2022-06   94776  11444  14770  14353  11199  7736  29832  5442  95622.0\n"," \n"," [69 rows x 9 columns],\n","          AHB_13     89        y\n"," 2016-10   23244  23244  23251.0\n"," 2016-11   23251  23251  23251.0\n"," 2016-12   23251  23251  23243.0\n"," 2017-01   23243  23243  23231.0\n"," 2017-02   23231  23231  23228.0\n"," ...         ...    ...      ...\n"," 2022-02   39325  39325  39613.0\n"," 2022-03   39613  39613  39933.0\n"," 2022-04   39933  39933  40963.0\n"," 2022-05   40963  40963  41324.0\n"," 2022-06   41324  41324  40870.0\n"," \n"," [69 rows x 3 columns]]"]},"metadata":{},"execution_count":112}],"source":["AHB_data"]},{"cell_type":"markdown","metadata":{"id":"r2RFcsUmJfvd"},"source":["#Deep Learning"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1663953246338,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"KxiLNrkFxf86"},"outputs":[],"source":["import torch.nn as nn\n","\n","class RNNModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n","        super(RNNModel, self).__init__()\n","\n","        # Defining the number of layers and the nodes in each layer\n","        self.hidden_dim = hidden_dim\n","        self.layer_dim = layer_dim\n","\n","        # RNN layers\n","        self.rnn = nn.RNN(\n","            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n","        )\n","        # Fully connected layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Initializing hidden state for first input with zeros\n","        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n","\n","        # Forward propagation by passing in the input and hidden state into the model\n","        out, h0 = self.rnn(x, h0.detach())\n","\n","        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n","        # so that it can fit into the fully connected layer\n","        out = out[:, -1, :]\n","\n","        # Convert the final state to our desired output shape (batch_size, output_dim)\n","        out = self.fc(out)\n","        return out"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1663953246338,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"5QJB7957x7YR"},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n","        super(LSTMModel, self).__init__()\n","\n","        # Defining the number of layers and the nodes in each layer\n","        self.hidden_dim = hidden_dim\n","        self.layer_dim = layer_dim\n","\n","        # LSTM layers\n","        self.lstm = nn.LSTM(\n","            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n","        )\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Initializing hidden state for first input with zeros\n","        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n","\n","        # Initializing cell state for first input with zeros\n","        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n","\n","        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n","        # If we don't, we'll backprop all the way to the start even after going through another batch\n","        # Forward propagation by passing in the input, hidden state, and cell state into the model\n","        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n","\n","        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n","        # so that it can fit into the fully connected layer\n","        out = out[:, -1, :]\n","\n","        # Convert the final state to our desired output shape (batch_size, output_dim)\n","        out = self.fc(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1663953246338,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"7AvPKwC-yH-8"},"outputs":[],"source":["class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n","        super(GRUModel, self).__init__()\n","\n","        # Defining the number of layers and the nodes in each layer\n","        self.layer_dim = layer_dim\n","        self.hidden_dim = hidden_dim\n","\n","        # GRU layers\n","        self.gru = nn.GRU(\n","            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n","        )\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Initializing hidden state for first input with zeros\n","        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n","\n","        # Forward propagation by passing in the input and hidden state into the model\n","        out, _ = self.gru(x, h0.detach())\n","\n","        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n","        # so that it can fit into the fully connected layer\n","        out = out[:, -1, :]\n","\n","        # Convert the final state to our desired output shape (batch_size, output_dim)\n","        out = self.fc(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1663953246338,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"gidDvpdlyL_3"},"outputs":[],"source":["def get_model(model, model_params):\n","    models = {\n","        \"rnn\": RNNModel,\n","        \"lstm\": LSTMModel,\n","        \"gru\": GRUModel,\n","    }\n","    return models.get(model.lower())(**model_params)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1663953246339,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"qBVkbcq1yQ4D"},"outputs":[],"source":["class Optimization:\n","    \"\"\"Optimization is a helper class that allows training, validation, prediction.\n","\n","    Optimization is a helper class that takes model, loss function, optimizer function\n","    learning scheduler (optional), early stopping (optional) as inputs. In return, it\n","    provides a framework to train and validate the models, and to predict future values\n","    based on the models.\n","\n","    Attributes:\n","        model (RNNModel, LSTMModel, GRUModel): Model class created for the type of RNN\n","        loss_fn (torch.nn.modules.Loss): Loss function to calculate the losses\n","        optimizer (torch.optim.Optimizer): Optimizer function to optimize the loss function\n","        train_losses (list[float]): The loss values from the training\n","        val_losses (list[float]): The loss values from the validation\n","        last_epoch (int): The number of epochs that the models is trained\n","    \"\"\"\n","    def __init__(self, model, loss_fn, optimizer):\n","        \"\"\"\n","        Args:\n","            model (RNNModel, LSTMModel, GRUModel): Model class created for the type of RNN\n","            loss_fn (torch.nn.modules.Loss): Loss function to calculate the losses\n","            optimizer (torch.optim.Optimizer): Optimizer function to optimize the loss function\n","        \"\"\"\n","        self.model = model\n","        self.loss_fn = loss_fn\n","        self.optimizer = optimizer\n","        self.train_losses = []\n","        self.val_losses = []\n","        self.last_epoch = 0\n","        \n","    def train_step(self, x, y):\n","        \"\"\"The method train_step completes one step of training.\n","\n","        Given the features (x) and the target values (y) tensors, the method completes\n","        one step of the training. First, it activates the train mode to enable back prop.\n","        After generating predicted values (yhat) by doing forward propagation, it calculates\n","        the losses by using the loss function. Then, it computes the gradients by doing\n","        back propagation and updates the weights by calling step() function.\n","\n","        Args:\n","            x (torch.Tensor): Tensor for features to train one step\n","            y (torch.Tensor): Tensor for target values to calculate losses\n","\n","        \"\"\"\n","        # Sets model to train mode\n","        self.model.train()\n","\n","        # Makes predictions\n","        yhat = self.model(x)\n","\n","        # Computes loss\n","        loss = self.loss_fn(y, yhat)\n","\n","        # Computes gradients\n","        loss.backward()\n","\n","        # Updates parameters and zeroes gradients\n","        self.optimizer.step()\n","        self.optimizer.zero_grad()\n","\n","        # Returns the loss\n","        return loss.item()\n","\n","    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1, the_last_loss = 100, patience = 2, trigger_times = 0):\n","        \"\"\"The method train performs the model training\n","\n","        The method takes DataLoaders for training and validation datasets, batch size for\n","        mini-batch training, number of epochs to train, and number of features as inputs.\n","        Then, it carries out the training by iteratively calling the method train_step for\n","        n_epochs times. If early stopping is enabled, then it  checks the stopping condition\n","        to decide whether the training needs to halt before n_epochs steps. Finally, it saves\n","        the model in a designated file path.\n","\n","        Args:\n","            train_loader (torch.utils.data.DataLoader): DataLoader that stores training data\n","            val_loader (torch.utils.data.DataLoader): DataLoader that stores validation data\n","            batch_size (int): Batch size for mini-batch training\n","            n_epochs (int): Number of epochs, i.e., train steps, to train\n","            n_features (int): Number of feature columns\n","\n","        \"\"\"\n","\n","        model_path = f'{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n","\n","        for epoch in range(1, n_epochs + 1):\n","            batch_losses = []\n","            for x_batch, y_batch in train_loader:\n","                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n","                y_batch = y_batch.to(device)\n","                loss = self.train_step(x_batch, y_batch)\n","                batch_losses.append(loss)\n","            training_loss = np.mean(batch_losses)\n","            self.train_losses.append(training_loss)\n","\n","            with torch.no_grad():\n","                batch_val_losses = []\n","                for x_val, y_val in val_loader:\n","                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n","                    y_val = y_val.to(device)\n","                    self.model.eval()\n","                    yhat = self.model(x_val)\n","                    val_loss = self.loss_fn(y_val, yhat).item()\n","                    batch_val_losses.append(val_loss)\n","                validation_loss = np.mean(batch_val_losses)\n","\n","\n","                current_loss = validation_loss\n","                #print('The Current Loss:', current_loss)\n","\n","                if current_loss > the_last_loss:\n","                    trigger_times = trigger_times + 1\n","                    #print('Trigger Times:', trigger_times)\n","\n","                    if trigger_times >= patience:\n","                        #print('Early Stopping!')\n","                        self.val_losses.append(validation_loss)\n","                        self.last_epoch = epoch \n","                        break\n","\n","\n","                else:\n","                    #print('Trigger Times: 0')\n","                    trigger_times = 0\n","                the_last_loss = current_loss\n","                self.val_losses.append(validation_loss)\n","            self.last_epoch = epoch \n","\n","            #if (epoch <= 10) | (epoch % 50 == 0):\n","            #    print(\n","            #        f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n","            #    )\n","\n","        #torch.save(self.model.state_dict(), model_path)\n","\n","    def evaluate(self, test_loader, batch_size=1, n_features=1):\n","        \"\"\"The method evaluate performs the model evaluation\n","\n","        The method takes DataLoaders for the test dataset, batch size for mini-batch testing,\n","        and number of features as inputs. Similar to the model validation, it iteratively\n","        predicts the target values and calculates losses. Then, it returns two lists that\n","        hold the predictions and the actual values.\n","\n","        Note:\n","            This method assumes that the prediction from the previous step is available at\n","            the time of the prediction, and only does one-step prediction into the future.\n","\n","        Args:\n","            test_loader (torch.utils.data.DataLoader): DataLoader that stores test data\n","            batch_size (int): Batch size for mini-batch training\n","            n_features (int): Number of feature columns\n","\n","        Returns:\n","            list[float]: The values predicted by the model\n","            list[float]: The actual values in the test set.\n","\n","        \"\"\"\n","        with torch.no_grad():\n","            predictions = []\n","            values = []\n","            for x_test, y_test in test_loader:\n","                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n","                y_test = y_test.to(device)\n","                self.model.eval()\n","                yhat = self.model(x_test)\n","                predictions.append(yhat.to(device).detach().numpy())\n","                values.append(y_test.to(device).detach().numpy())\n","\n","        return predictions, values\n","\n","    def plot_losses(self):\n","        \"\"\"The method plots the calculated loss values for training and validation\n","        \"\"\"\n","        plt.plot(self.train_losses, label=\"Training loss\")\n","        plt.plot(self.val_losses, label=\"Validation loss\")\n","        plt.legend()\n","        plt.title(\"Losses\")\n","        plt.show()\n","        plt.close()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1663953246339,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"ye7dsvZ3Pbxw"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n","\n","def get_scaler(scaler):\n","    scalers = {\n","        \"minmax\": MinMaxScaler,\n","        \"standard\": StandardScaler,\n","        \"maxabs\": MaxAbsScaler,\n","        \"robust\": RobustScaler,\n","    }\n","    return scalers.get(scaler.lower())()"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1663953246339,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"},"user_tz":-420},"id":"6pI2-i2CX1kh"},"outputs":[],"source":["def inverse_transform(scaler, df, columns):\n","    for col in columns:\n","        df[col] = scaler2.inverse_transform(df[col])\n","    return df\n","\n","\n","def format_predictions(predictions, values, df_test, scaler):\n","    vals = np.concatenate(values, axis=0).ravel()\n","    preds = np.concatenate(predictions, axis=0).ravel()\n","    df_result = pd.DataFrame(data={\"value\": vals, \"prediction\": preds}, index=df_test.head(len(vals)).index)\n","    df_result = df_result.sort_index()\n","    # df_result = inverse_transform(scaler, df_result, [[\"value\", \"prediction\"]])\n","    return df_result"]},{"cell_type":"code","source":["AHB_numbers = [0] # q\n","validate_sizes = [1] # i\n","test_sizes = [0.1] # j \n","hidden_dims = [8] # l\n","layer_dims = [1] # m\n","dropouts = [0.2] # n\n","n_epochss = [1000] # o\n","models = ['gru'] # p\n","patience = 3\n","for q in range(len(AHB_numbers)):\n","    for i in range(len(validate_sizes)):\n","        for j in range(len(test_sizes)):\n","            for l in range(len(hidden_dims)):\n","                for m in range(len(layer_dims)):\n","                    for n in range(len(dropouts)):\n","                        for o in range(len(n_epochss)):\n","                            for p in range(len(models)):\n","\n","                                X = AHB_data[AHB_numbers[q]].drop(['y','AHB_'+str(AHB_numbers[q]+1)], axis=1)\n","                                y = AHB_data[AHB_numbers[q]].drop(AHB_data[AHB_numbers[q]].columns.difference(['y']), axis=1) \n","\n","                                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_sizes[j], train_size=1-test_sizes[j], shuffle=False, stratify=None)\n","\n","                                X_val = pd.DataFrame(X_train.tail(validate_sizes[i]))\n","                                X_train = X_train.drop(X_train.tail(validate_sizes[i]).index)\n","                                y_val = pd.DataFrame(y_train.tail(validate_sizes[i]))\n","                                y_train = y_train.drop(y_train.tail(validate_sizes[i]).index)\n","\n","\n","\n","                                scaler = get_scaler('minmax')\n","                                scaler2 = get_scaler('minmax')\n","\n","                                X_train_arr = scaler.fit_transform(X_train)\n","                                X_val_arr = scaler.transform(X_val)\n","\n","                                y_train_arr = scaler2.fit_transform(y_train)\n","                                y_val_arr = scaler2.transform(y_val)\n","\n","\n","                                train_features = torch.Tensor(X_train_arr)\n","                                train_targets = torch.Tensor(y_train_arr)\n","                                val_features = torch.Tensor(X_val_arr)\n","                                val_targets = torch.Tensor(y_val_arr)\n","\n","                                train = TensorDataset(train_features, train_targets)\n","                                val = TensorDataset(val_features, val_targets)\n","\n","\n","                                input_dim = len(X.columns)\n","                                output_dim = 1\n","                                hidden_dim = hidden_dims[l]\n","                                layer_dim = layer_dims[m]\n","                                batch_size = 1\n","                                dropout = dropouts[n]\n","                                n_epochs = n_epochss[o]\n","                                learning_rate = 1e-3\n","                                weight_decay = 1e-6\n","                                patience = patience\n","\n","                                model_params = {'input_dim': input_dim,\n","                                                'hidden_dim' : hidden_dim,\n","                                                'layer_dim' : layer_dim,\n","                                                'output_dim' : output_dim,\n","                                                'dropout_prob' : dropout}\n","\n","                                model = get_model(models[p], model_params)\n","\n","                                loss_fn = nn.MSELoss(reduction=\"mean\")\n","                                optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","                                train_loader = DataLoader(train, batch_size=5, shuffle=False, drop_last=True)\n","                                val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)"],"metadata":{"id":"cIFQt4uxWz7Y","executionInfo":{"status":"ok","timestamp":1663954015676,"user_tz":-420,"elapsed":3,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["            for x_batch, y_batch in train_loader:\n","                x_batch = x_batch.view([5, -1, input_dim]).to(device)\n","                print(x_batch)\n","                #print(y_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bigrwwaXbKP","executionInfo":{"status":"ok","timestamp":1663954025846,"user_tz":-420,"elapsed":827,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}},"outputId":"15f3ad17-3619-4469-c0db-0cfed07c54f6"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[7.3121e-05, 0.0000e+00, 9.9239e-04, 0.0000e+00, 5.3176e-04,\n","          2.1631e-04, 9.5727e-02, 3.4219e-04]],\n","\n","        [[7.3121e-05, 0.0000e+00, 8.2699e-04, 0.0000e+00, 4.6529e-04,\n","          2.1631e-04, 9.5727e-02, 3.4219e-04]],\n","\n","        [[0.0000e+00, 0.0000e+00, 8.2699e-04, 0.0000e+00, 3.9882e-04,\n","          2.1631e-04, 9.5727e-02, 3.4219e-04]],\n","\n","        [[0.0000e+00, 8.3436e-02, 8.2699e-04, 0.0000e+00, 3.3235e-05,\n","          2.1631e-04, 9.5727e-02, 2.2813e-04]],\n","\n","        [[0.0000e+00, 8.3436e-02, 6.6159e-04, 0.0000e+00, 0.0000e+00,\n","          2.1631e-04, 9.5727e-02, 2.2813e-04]]])\n","tensor([[[1.9472e-01, 8.3436e-02, 6.6159e-04, 0.0000e+00, 2.0606e-02,\n","          2.1631e-04, 1.7713e-01, 2.2813e-04]],\n","\n","        [[1.9472e-01, 1.8135e-01, 6.6159e-04, 0.0000e+00, 2.0938e-02,\n","          2.1631e-04, 1.7713e-01, 3.4219e-04]],\n","\n","        [[1.9472e-01, 2.0491e-01, 4.9620e-04, 0.0000e+00, 2.0905e-02,\n","          2.1631e-04, 1.7713e-01, 3.4219e-04]],\n","\n","        [[1.9472e-01, 2.0491e-01, 4.9620e-04, 0.0000e+00, 2.0871e-02,\n","          2.1631e-04, 2.0171e-01, 3.4219e-04]],\n","\n","        [[1.9465e-01, 2.0466e-01, 1.6540e-04, 0.0000e+00, 2.0340e-02,\n","          2.1631e-04, 2.1468e-01, 2.2813e-04]]])\n","tensor([[[1.9465e-01, 2.0466e-01, 1.6540e-04, 4.4631e-02, 2.0705e-02,\n","          2.1631e-04, 2.1468e-01, 2.2813e-04]],\n","\n","        [[1.9457e-01, 2.0466e-01, 0.0000e+00, 1.2794e-01, 2.0506e-02,\n","          0.0000e+00, 2.1468e-01, 2.2813e-04]],\n","\n","        [[1.9457e-01, 3.2221e-01, 0.0000e+00, 1.2794e-01, 2.0473e-02,\n","          0.0000e+00, 2.1445e-01, 1.1406e-04]],\n","\n","        [[1.9450e-01, 3.2172e-01, 2.0675e-02, 1.2821e-01, 2.0340e-02,\n","          0.0000e+00, 2.1445e-01, 1.1406e-04]],\n","\n","        [[2.0518e-01, 3.2172e-01, 7.3933e-02, 1.2821e-01, 2.0173e-02,\n","          0.0000e+00, 2.1434e-01, 1.1406e-04]]])\n","tensor([[[3.3489e-01, 3.2147e-01, 1.0056e-01, 1.2821e-01, 1.9343e-02,\n","          0.0000e+00, 2.1434e-01, 1.1406e-04]],\n","\n","        [[3.3760e-01, 3.2147e-01, 4.5038e-01, 1.2821e-01, 1.8911e-02,\n","          0.0000e+00, 2.1434e-01, 1.1406e-04]],\n","\n","        [[3.3753e-01, 3.2123e-01, 4.5154e-01, 1.2821e-01, 1.8944e-02,\n","          0.0000e+00, 2.1434e-01, 1.1406e-04]],\n","\n","        [[3.3753e-01, 4.2896e-01, 4.5418e-01, 1.2821e-01, 1.8944e-02,\n","          0.0000e+00, 2.1434e-01, 1.1406e-04]],\n","\n","        [[3.3694e-01, 4.5644e-01, 4.5352e-01, 4.5009e-01, 5.0537e-01,\n","          4.2007e-01, 2.1434e-01, 0.0000e+00]]])\n","tensor([[[0.3367, 0.4564, 0.4535, 0.4501, 0.5054, 0.4199, 0.2142, 0.0000]],\n","\n","        [[0.3367, 0.4564, 0.4572, 0.4501, 0.5054, 0.4199, 0.2141, 0.4299]],\n","\n","        [[0.3367, 0.4564, 0.4572, 0.4501, 0.5054, 0.4199, 0.2141, 0.4299]],\n","\n","        [[0.3556, 0.5421, 0.4588, 0.4501, 0.5051, 0.4199, 0.2140, 0.4299]],\n","\n","        [[0.3555, 0.5418, 0.4585, 0.6137, 0.5050, 0.4196, 0.2140, 0.4811]]])\n","tensor([[[0.3555, 0.5418, 0.4585, 0.6137, 0.5050, 0.4196, 0.2140, 0.4958]],\n","\n","        [[0.3557, 0.5418, 0.4582, 0.6137, 0.5048, 0.4199, 0.2138, 0.4958]],\n","\n","        [[0.3556, 0.6079, 0.4582, 0.6137, 0.5048, 0.4196, 0.2138, 0.4970]],\n","\n","        [[0.3554, 0.6076, 0.4722, 0.6137, 0.5048, 0.4196, 0.2138, 0.4970]],\n","\n","        [[0.3554, 0.6074, 0.4735, 0.6137, 0.5047, 0.4196, 0.2138, 0.4970]]])\n","tensor([[[0.3553, 0.6479, 0.4735, 0.6137, 0.5047, 0.4196, 0.2138, 0.4970]],\n","\n","        [[0.3553, 0.6479, 0.4740, 0.6137, 0.5046, 0.4196, 0.4088, 0.4970]],\n","\n","        [[0.3553, 0.6479, 0.4749, 0.6135, 0.5046, 0.4196, 0.4077, 0.4970]],\n","\n","        [[0.3553, 0.6476, 0.4749, 0.6135, 0.5045, 0.4196, 0.4077, 0.4970]],\n","\n","        [[0.3553, 0.6476, 0.4749, 0.6135, 0.5045, 0.4196, 0.4077, 0.4970]]])\n","tensor([[[0.3548, 0.7274, 0.4750, 0.6135, 0.5036, 0.4194, 0.4070, 0.4970]],\n","\n","        [[0.6171, 0.5470, 0.6229, 0.6186, 0.6182, 0.6385, 0.3076, 0.6451]],\n","\n","        [[0.6353, 0.5654, 0.6389, 0.6351, 0.6282, 0.6537, 0.3283, 0.6562]],\n","\n","        [[0.6672, 0.6238, 0.6785, 0.6808, 0.7070, 0.6950, 0.3639, 0.6829]],\n","\n","        [[0.6808, 0.6410, 0.6907, 0.6984, 0.7242, 0.7236, 0.3765, 0.6959]]])\n","tensor([[[0.6941, 0.6562, 0.7091, 0.7119, 0.7359, 0.7348, 0.3844, 0.7047]],\n","\n","        [[0.7186, 0.6800, 0.7302, 0.7406, 0.7472, 0.7564, 0.3961, 0.7260]],\n","\n","        [[0.7399, 0.7308, 0.7653, 0.7522, 0.7619, 0.7681, 0.0000, 0.7723]],\n","\n","        [[0.7521, 0.7546, 0.7789, 0.7649, 0.7728, 0.7791, 0.0232, 0.7897]],\n","\n","        [[0.7649, 0.7693, 0.7933, 0.7796, 0.7831, 0.7913, 0.5965, 0.8036]]])\n","tensor([[[0.7831, 0.7821, 0.8010, 0.7898, 0.7922, 0.8027, 0.8392, 0.8095]],\n","\n","        [[0.7955, 0.7931, 0.8128, 0.8012, 0.8042, 0.8131, 0.8453, 0.8243]],\n","\n","        [[0.8079, 0.8096, 0.8252, 0.8174, 0.8138, 0.8334, 0.8588, 0.8380]],\n","\n","        [[0.8233, 0.8275, 0.8379, 0.8326, 0.8240, 0.8388, 0.8695, 0.8493]],\n","\n","        [[0.8373, 0.8444, 0.8530, 0.8485, 0.8487, 0.8637, 0.8885, 0.8623]]])\n","tensor([[[0.8550, 0.8596, 0.8670, 0.8637, 0.8705, 0.8864, 0.9024, 0.8746]],\n","\n","        [[0.8742, 0.8726, 0.8788, 0.8859, 0.8837, 0.8970, 0.9102, 0.8880]],\n","\n","        [[0.8870, 0.8842, 0.8981, 0.8959, 0.8901, 0.9111, 0.9178, 0.9032]],\n","\n","        [[0.9030, 0.9058, 0.9112, 0.9143, 0.9106, 0.9275, 0.9351, 0.9184]],\n","\n","        [[0.9131, 0.9190, 0.9289, 0.9272, 0.9246, 0.9355, 0.9442, 0.9291]]])\n","tensor([[[0.9270, 0.9281, 0.9383, 0.9381, 0.9320, 0.9453, 0.9543, 0.9417]],\n","\n","        [[0.9407, 0.9399, 0.9491, 0.9483, 0.9320, 0.9563, 0.9582, 0.9536]],\n","\n","        [[0.9612, 0.9512, 0.9628, 0.9570, 0.9611, 0.9725, 0.9717, 0.9676]],\n","\n","        [[0.9708, 0.9512, 0.9757, 0.9708, 0.9737, 0.9725, 0.9811, 0.9812]],\n","\n","        [[0.9864, 0.9831, 0.9940, 0.9773, 0.9879, 0.9974, 0.9947, 0.9926]]])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1GJ0mkybzzTnpEiU42P_-jZoIyK3a430z"},"id":"JppbBFUMvgf5","executionInfo":{"status":"ok","timestamp":1663721291737,"user_tz":-420,"elapsed":23205947,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}},"outputId":"8bb580c7-4cc2-4580-fbc8-79abeb1ae2ee"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["%matplotlib inline\n","AHB_numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 , 12] # q\n","validate_sizes = [1] # i\n","test_sizes = [0.1] # j \n","hidden_dims = [8, 16, 32, 64, 128] # l\n","layer_dims = [1, 2, 3] # m\n","dropouts = [0.2, 0.3, 0.5] # n\n","n_epochss = [1000] # o\n","models = ['gru', 'lstm', 'rnn'] # p\n","patience = 3\n","\n","count = 1\n","performance_json = []\n","for q in range(len(AHB_numbers)):\n","    for i in range(len(validate_sizes)):\n","        for j in range(len(test_sizes)):\n","            for l in range(len(hidden_dims)):\n","                for m in range(len(layer_dims)):\n","                    for n in range(len(dropouts)):\n","                        for o in range(len(n_epochss)):\n","                            for p in range(len(models)):\n","\n","                                print('PROGRESS: [ ' + str(count) + ' / ' + str(len(AHB_numbers)*len(validate_sizes)*len(test_sizes)*len(hidden_dims)*len(layer_dims)*len(dropouts)*len(n_epochss)*len(models)) +' ]')\n","                                \n","                                # Univariate\n","                                X = AHB_data[AHB_numbers[q]].drop(AHB_data[AHB_numbers[q]].columns.difference(['AHB_'+str(AHB_numbers[q]+1)]), axis=1) \n","                                y = AHB_data[AHB_numbers[q]].drop(AHB_data[AHB_numbers[q]].columns.difference(['y']), axis=1) \n","\n","                                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_sizes[j], train_size=1-test_sizes[j], shuffle=False, stratify=None)\n","\n","                                X_val = pd.DataFrame(X_train.tail(validate_sizes[i]))\n","                                X_train = X_train.drop(X_train.tail(validate_sizes[i]).index)\n","                                y_val = pd.DataFrame(y_train.tail(validate_sizes[i]))\n","                                y_train = y_train.drop(y_train.tail(validate_sizes[i]).index)\n","\n","\n","\n","                                scaler = get_scaler('minmax')\n","                                scaler2 = get_scaler('minmax')\n","\n","                                X_train_arr = scaler.fit_transform(X_train)\n","                                X_val_arr = scaler.transform(X_val)\n","\n","                                y_train_arr = scaler2.fit_transform(y_train)\n","                                y_val_arr = scaler2.transform(y_val)\n","\n","\n","                                train_features = torch.Tensor(X_train_arr)\n","                                train_targets = torch.Tensor(y_train_arr)\n","                                val_features = torch.Tensor(X_val_arr)\n","                                val_targets = torch.Tensor(y_val_arr)\n","\n","                                train = TensorDataset(train_features, train_targets)\n","                                val = TensorDataset(val_features, val_targets)\n","\n","\n","                                input_dim = len(X.columns)\n","                                output_dim = 1\n","                                hidden_dim = hidden_dims[l]\n","                                layer_dim = layer_dims[m]\n","                                batch_size = 1\n","                                dropout = dropouts[n]\n","                                n_epochs = n_epochss[o]\n","                                learning_rate = 1e-3\n","                                weight_decay = 1e-6\n","                                patience = patience\n","\n","                                model_params = {'input_dim': input_dim,\n","                                                'hidden_dim' : hidden_dim,\n","                                                'layer_dim' : layer_dim,\n","                                                'output_dim' : output_dim,\n","                                                'dropout_prob' : dropout}\n","\n","                                model = get_model(models[p], model_params)\n","\n","                                loss_fn = nn.MSELoss(reduction=\"mean\")\n","                                optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","                                train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n","                                val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n","\n","                                opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n","                                opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim, patience=patience)\n","                                opt.plot_losses()\n","\n","                                X_test_arr = scaler.transform(X_test)\n","                                y_test_arr = scaler2.transform(y_test)\n","\n","                                test_features = torch.Tensor(X_test_arr)\n","                                test_targets = torch.Tensor(y_test_arr)\n","\n","                                test = TensorDataset(test_features, test_targets)\n","\n","                                test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n","                                test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)\n","\n","\n","                                predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)\n","                                df_result = format_predictions(predictions, values, X_test, scaler)\n","                                df_result_inverse = scaler2.inverse_transform(df_result)\n","                                df_result_inverse = pd.DataFrame(df_result_inverse, index=df_result.index, columns=df_result.columns)\n","\n","                                mae = mean_absolute_error(df_result.value, df_result.prediction)\n","                                mse = mean_squared_error(df_result.value, df_result.prediction)\n","                                rmse = mean_squared_error(df_result.value, df_result.prediction, squared = False)\n","                                mape = mean_absolute_percentage_error(df_result.value, df_result.prediction)\n","                                \n","                                mae_inverse = mean_absolute_error(df_result_inverse.value, df_result_inverse.prediction)\n","                                mse_inverse = mean_squared_error(df_result_inverse.value, df_result_inverse.prediction)\n","                                rmse_inverse = mean_squared_error(df_result_inverse.value, df_result_inverse.prediction, squared = False)\n","                                mape_inverse = mean_absolute_percentage_error(df_result_inverse.value, df_result_inverse.prediction)\n","\n","\n","\n","                                performance_json.append({'AHB_number': AHB_numbers[q]+1, 'Multi_Uni': 'Uni', 'validate_size': validate_sizes[i], \\\n","                                                         'test_size': test_sizes[j], 'hidden_dim': hidden_dims[l], 'layer_dim':layer_dims[m], \\\n","                                                         'dropout': dropouts[n], 'n_epochs': n_epochss[o], 'last_epoch': opt.last_epoch,'model': models[p],'MAE_normalize': float(mae), \\\n","                                                         'MSE_normalize': float(mse), 'RMSE_normalize': float(rmse), 'MAPE_normalize': float(mape), \\\n","                                                         'MAE': float(mae_inverse), 'MSE': float(mse_inverse), 'RMSE': float(rmse_inverse), 'MAPE': float(mape_inverse)})\n","                                \n","\n","                                json_formatted_str = json.dumps(performance_json[-1], indent=2)\n","                                print(json_formatted_str)\n","\n","\n","\n","                                # Multivariate\n","                                X = AHB_data[AHB_numbers[q]].drop(['y','AHB_'+str(AHB_numbers[q]+1)], axis=1)\n","                                y = AHB_data[AHB_numbers[q]].drop(AHB_data[AHB_numbers[q]].columns.difference(['y']), axis=1) \n","\n","                                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_sizes[j], train_size=1-test_sizes[j], shuffle=False, stratify=None)\n","\n","                                X_val = pd.DataFrame(X_train.tail(validate_sizes[i]))\n","                                X_train = X_train.drop(X_train.tail(validate_sizes[i]).index)\n","                                y_val = pd.DataFrame(y_train.tail(validate_sizes[i]))\n","                                y_train = y_train.drop(y_train.tail(validate_sizes[i]).index)\n","\n","\n","\n","                                scaler = get_scaler('minmax')\n","                                scaler2 = get_scaler('minmax')\n","\n","                                X_train_arr = scaler.fit_transform(X_train)\n","                                X_val_arr = scaler.transform(X_val)\n","\n","                                y_train_arr = scaler2.fit_transform(y_train)\n","                                y_val_arr = scaler2.transform(y_val)\n","\n","\n","                                train_features = torch.Tensor(X_train_arr)\n","                                train_targets = torch.Tensor(y_train_arr)\n","                                val_features = torch.Tensor(X_val_arr)\n","                                val_targets = torch.Tensor(y_val_arr)\n","\n","                                train = TensorDataset(train_features, train_targets)\n","                                val = TensorDataset(val_features, val_targets)\n","\n","\n","                                input_dim = len(X.columns)\n","                                output_dim = 1\n","                                hidden_dim = hidden_dims[l]\n","                                layer_dim = layer_dims[m]\n","                                batch_size = 1\n","                                dropout = dropouts[n]\n","                                n_epochs = n_epochss[o]\n","                                learning_rate = 1e-3\n","                                weight_decay = 1e-6\n","                                patience = patience\n","\n","                                model_params = {'input_dim': input_dim,\n","                                                'hidden_dim' : hidden_dim,\n","                                                'layer_dim' : layer_dim,\n","                                                'output_dim' : output_dim,\n","                                                'dropout_prob' : dropout}\n","\n","                                model = get_model(models[p], model_params)\n","\n","                                loss_fn = nn.MSELoss(reduction=\"mean\")\n","                                optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","                                train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n","                                val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n","\n","                                opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n","                                opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim, patience=patience)\n","                                opt.plot_losses()\n","\n","                                X_test_arr = scaler.transform(X_test)\n","                                y_test_arr = scaler2.transform(y_test)\n","\n","                                test_features = torch.Tensor(X_test_arr)\n","                                test_targets = torch.Tensor(y_test_arr)\n","\n","                                test = TensorDataset(test_features, test_targets)\n","\n","                                test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n","                                test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)\n","\n","\n","                                predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)\n","                                df_result = format_predictions(predictions, values, X_test, scaler)\n","                                df_result_inverse = scaler2.inverse_transform(df_result)\n","                                df_result_inverse = pd.DataFrame(df_result_inverse, index=df_result.index, columns=df_result.columns)\n","\n","                                mae = mean_absolute_error(df_result.value, df_result.prediction)\n","                                mse = mean_squared_error(df_result.value, df_result.prediction)\n","                                rmse = mean_squared_error(df_result.value, df_result.prediction, squared = False)\n","                                mape = mean_absolute_percentage_error(df_result.value, df_result.prediction)\n","                                \n","                                mae_inverse = mean_absolute_error(df_result_inverse.value, df_result_inverse.prediction)\n","                                mse_inverse = mean_squared_error(df_result_inverse.value, df_result_inverse.prediction)\n","                                rmse_inverse = mean_squared_error(df_result_inverse.value, df_result_inverse.prediction, squared = False)\n","                                mape_inverse = mean_absolute_percentage_error(df_result_inverse.value, df_result_inverse.prediction)\n","\n","\n","\n","                                performance_json.append({'AHB_number': AHB_numbers[q]+1, 'Multi_Uni': 'Multi', 'validate_size': validate_sizes[i], \\\n","                                                         'test_size': test_sizes[j], 'hidden_dim': hidden_dims[l], 'layer_dim':layer_dims[m], \\\n","                                                         'dropout': dropouts[n], 'n_epochs': n_epochss[o], 'last_epoch': opt.last_epoch,'model': models[p],'MAE_normalize': float(mae), \\\n","                                                         'MSE_normalize': float(mse), 'RMSE_normalize': float(rmse), 'MAPE_normalize': float(mape), \\\n","                                                         'MAE': float(mae_inverse), 'MSE': float(mse_inverse), 'RMSE': float(rmse_inverse), 'MAPE': float(mape_inverse)})\n","                                \n","\n","                                json_formatted_str = json.dumps(performance_json[-1], indent=2)\n","                                print(json_formatted_str)\n","                                count = count + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2f6Tihut6qy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663721304657,"user_tz":-420,"elapsed":908,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}},"outputId":"b2eb62fa-4ea0-489e-cdb4-a9a6130b424b"},"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/all_performance_json_no_cross_val.json': No such file or directory\n"]}],"source":["json.dump(performance_json, open(\"all_performance_jsonno_no_cross_val.json\",\"w\"))\n","df_json = pd.read_json('/content/all_performance_jsonno_no_cross_val.json')\n","df_json.to_csv('/content/all_performance_json_no_cross_val.csv', index=False)\n","! cp /content/all_performance_json_no_cross_val.json /content/drive/MyDrive/Depression\n","! cp /content/all_performance_json_no_cross_val.csv /content/drive/MyDrive/Depression"]},{"cell_type":"code","source":["df_json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"81BQjhO3kkhP","executionInfo":{"status":"ok","timestamp":1663721304658,"user_tz":-420,"elapsed":191,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}},"outputId":"e6781611-c42e-4f82-eef5-e97a6146275b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      AHB_number Multi_Uni  validate_size  test_size  hidden_dim  layer_dim  \\\n","0              1       Uni              1        0.1           8          1   \n","1              1     Multi              1        0.1           8          1   \n","2              1       Uni              1        0.1           8          1   \n","3              1     Multi              1        0.1           8          1   \n","4              1       Uni              1        0.1           8          1   \n","...          ...       ...            ...        ...         ...        ...   \n","3505          13     Multi              1        0.1         128          3   \n","3506          13       Uni              1        0.1         128          3   \n","3507          13     Multi              1        0.1         128          3   \n","3508          13       Uni              1        0.1         128          3   \n","3509          13     Multi              1        0.1         128          3   \n","\n","      dropout  n_epochs  last_epoch model  MAE_normalize  MSE_normalize  \\\n","0         0.2      1000         113   gru       0.012597       0.000195   \n","1         0.2      1000         232   gru       0.003308       0.000016   \n","2         0.2      1000          24  lstm       0.015367       0.000285   \n","3         0.2      1000          12  lstm       0.046470       0.002270   \n","4         0.2      1000          56   rnn       0.010015       0.000133   \n","...       ...       ...         ...   ...            ...            ...   \n","3505      0.5      1000           5   gru       0.136841       0.019606   \n","3506      0.5      1000          27  lstm       0.085525       0.008024   \n","3507      0.5      1000          86  lstm       0.074214       0.006258   \n","3508      0.5      1000          14   rnn       0.119502       0.015163   \n","3509      0.5      1000           8   rnn       0.062869       0.004725   \n","\n","      RMSE_normalize  MAPE_normalize          MAE           MSE         RMSE  \\\n","0           0.013967        0.011678  1008.799133  1.250944e+06  1118.456299   \n","1           0.003993        0.003074   264.906250  1.022384e+05   319.747467   \n","2           0.016895        0.014241  1230.573608  1.830430e+06  1352.933960   \n","3           0.047639        0.043272  3721.303467  1.455400e+07  3814.970459   \n","4           0.011538        0.009270   801.986633  8.536719e+05   923.943665   \n","...              ...             ...          ...           ...          ...   \n","3505        0.140020        0.127863  2166.877686  4.916034e+06  2217.213135   \n","3506        0.089574        0.079134  1354.288452  2.011872e+06  1418.404663   \n","3507        0.079108        0.068381  1175.177490  1.569210e+06  1252.681030   \n","3508        0.123140        0.110432  1892.311890  3.802192e+06  1949.921143   \n","3509        0.068735        0.057778   995.533508  1.184660e+06  1088.421021   \n","\n","          MAPE  \n","0     0.007203  \n","1     0.001894  \n","2     0.008785  \n","3     0.026643  \n","4     0.005721  \n","...        ...  \n","3505  0.053911  \n","3506  0.033555  \n","3507  0.029066  \n","3508  0.046860  \n","3509  0.024596  \n","\n","[3510 rows x 18 columns]"],"text/html":["\n","  <div id=\"df-5a722e19-0c93-4177-a53e-19c1e9305bab\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>AHB_number</th>\n","      <th>Multi_Uni</th>\n","      <th>validate_size</th>\n","      <th>test_size</th>\n","      <th>hidden_dim</th>\n","      <th>layer_dim</th>\n","      <th>dropout</th>\n","      <th>n_epochs</th>\n","      <th>last_epoch</th>\n","      <th>model</th>\n","      <th>MAE_normalize</th>\n","      <th>MSE_normalize</th>\n","      <th>RMSE_normalize</th>\n","      <th>MAPE_normalize</th>\n","      <th>MAE</th>\n","      <th>MSE</th>\n","      <th>RMSE</th>\n","      <th>MAPE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Uni</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0.2</td>\n","      <td>1000</td>\n","      <td>113</td>\n","      <td>gru</td>\n","      <td>0.012597</td>\n","      <td>0.000195</td>\n","      <td>0.013967</td>\n","      <td>0.011678</td>\n","      <td>1008.799133</td>\n","      <td>1.250944e+06</td>\n","      <td>1118.456299</td>\n","      <td>0.007203</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Multi</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0.2</td>\n","      <td>1000</td>\n","      <td>232</td>\n","      <td>gru</td>\n","      <td>0.003308</td>\n","      <td>0.000016</td>\n","      <td>0.003993</td>\n","      <td>0.003074</td>\n","      <td>264.906250</td>\n","      <td>1.022384e+05</td>\n","      <td>319.747467</td>\n","      <td>0.001894</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Uni</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0.2</td>\n","      <td>1000</td>\n","      <td>24</td>\n","      <td>lstm</td>\n","      <td>0.015367</td>\n","      <td>0.000285</td>\n","      <td>0.016895</td>\n","      <td>0.014241</td>\n","      <td>1230.573608</td>\n","      <td>1.830430e+06</td>\n","      <td>1352.933960</td>\n","      <td>0.008785</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>Multi</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0.2</td>\n","      <td>1000</td>\n","      <td>12</td>\n","      <td>lstm</td>\n","      <td>0.046470</td>\n","      <td>0.002270</td>\n","      <td>0.047639</td>\n","      <td>0.043272</td>\n","      <td>3721.303467</td>\n","      <td>1.455400e+07</td>\n","      <td>3814.970459</td>\n","      <td>0.026643</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>Uni</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0.2</td>\n","      <td>1000</td>\n","      <td>56</td>\n","      <td>rnn</td>\n","      <td>0.010015</td>\n","      <td>0.000133</td>\n","      <td>0.011538</td>\n","      <td>0.009270</td>\n","      <td>801.986633</td>\n","      <td>8.536719e+05</td>\n","      <td>923.943665</td>\n","      <td>0.005721</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3505</th>\n","      <td>13</td>\n","      <td>Multi</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>128</td>\n","      <td>3</td>\n","      <td>0.5</td>\n","      <td>1000</td>\n","      <td>5</td>\n","      <td>gru</td>\n","      <td>0.136841</td>\n","      <td>0.019606</td>\n","      <td>0.140020</td>\n","      <td>0.127863</td>\n","      <td>2166.877686</td>\n","      <td>4.916034e+06</td>\n","      <td>2217.213135</td>\n","      <td>0.053911</td>\n","    </tr>\n","    <tr>\n","      <th>3506</th>\n","      <td>13</td>\n","      <td>Uni</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>128</td>\n","      <td>3</td>\n","      <td>0.5</td>\n","      <td>1000</td>\n","      <td>27</td>\n","      <td>lstm</td>\n","      <td>0.085525</td>\n","      <td>0.008024</td>\n","      <td>0.089574</td>\n","      <td>0.079134</td>\n","      <td>1354.288452</td>\n","      <td>2.011872e+06</td>\n","      <td>1418.404663</td>\n","      <td>0.033555</td>\n","    </tr>\n","    <tr>\n","      <th>3507</th>\n","      <td>13</td>\n","      <td>Multi</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>128</td>\n","      <td>3</td>\n","      <td>0.5</td>\n","      <td>1000</td>\n","      <td>86</td>\n","      <td>lstm</td>\n","      <td>0.074214</td>\n","      <td>0.006258</td>\n","      <td>0.079108</td>\n","      <td>0.068381</td>\n","      <td>1175.177490</td>\n","      <td>1.569210e+06</td>\n","      <td>1252.681030</td>\n","      <td>0.029066</td>\n","    </tr>\n","    <tr>\n","      <th>3508</th>\n","      <td>13</td>\n","      <td>Uni</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>128</td>\n","      <td>3</td>\n","      <td>0.5</td>\n","      <td>1000</td>\n","      <td>14</td>\n","      <td>rnn</td>\n","      <td>0.119502</td>\n","      <td>0.015163</td>\n","      <td>0.123140</td>\n","      <td>0.110432</td>\n","      <td>1892.311890</td>\n","      <td>3.802192e+06</td>\n","      <td>1949.921143</td>\n","      <td>0.046860</td>\n","    </tr>\n","    <tr>\n","      <th>3509</th>\n","      <td>13</td>\n","      <td>Multi</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>128</td>\n","      <td>3</td>\n","      <td>0.5</td>\n","      <td>1000</td>\n","      <td>8</td>\n","      <td>rnn</td>\n","      <td>0.062869</td>\n","      <td>0.004725</td>\n","      <td>0.068735</td>\n","      <td>0.057778</td>\n","      <td>995.533508</td>\n","      <td>1.184660e+06</td>\n","      <td>1088.421021</td>\n","      <td>0.024596</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3510 rows × 18 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a722e19-0c93-4177-a53e-19c1e9305bab')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5a722e19-0c93-4177-a53e-19c1e9305bab button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5a722e19-0c93-4177-a53e-19c1e9305bab');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":126}]},{"cell_type":"code","source":["df_json_gru = df_json[df_json['model'] == 'gru']\n","df_json_gru[df_json_gru['MAE_normalize'] == df_json_gru['MAE_normalize'].min()]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"6uNdgsX0kyJx","executionInfo":{"status":"ok","timestamp":1663721304662,"user_tz":-420,"elapsed":193,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}},"outputId":"8d9cd387-dfdc-4d28-fc7d-3713b1e005c4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      AHB_number Multi_Uni  validate_size  test_size  hidden_dim  layer_dim  \\\n","2538          10       Uni              1        0.1          32          1   \n","\n","      dropout  n_epochs  last_epoch model  MAE_normalize  MSE_normalize  \\\n","2538      0.2      1000          15   gru       0.002984       0.000012   \n","\n","      RMSE_normalize  MAPE_normalize         MAE           MSE        RMSE  \\\n","2538        0.003439        0.002819  153.560272  31333.800781  177.013565   \n","\n","          MAPE  \n","2538  0.001501  "],"text/html":["\n","  <div id=\"df-df7245e6-a3b8-4465-b661-34d697ce3d34\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>AHB_number</th>\n","      <th>Multi_Uni</th>\n","      <th>validate_size</th>\n","      <th>test_size</th>\n","      <th>hidden_dim</th>\n","      <th>layer_dim</th>\n","      <th>dropout</th>\n","      <th>n_epochs</th>\n","      <th>last_epoch</th>\n","      <th>model</th>\n","      <th>MAE_normalize</th>\n","      <th>MSE_normalize</th>\n","      <th>RMSE_normalize</th>\n","      <th>MAPE_normalize</th>\n","      <th>MAE</th>\n","      <th>MSE</th>\n","      <th>RMSE</th>\n","      <th>MAPE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2538</th>\n","      <td>10</td>\n","      <td>Uni</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>0.2</td>\n","      <td>1000</td>\n","      <td>15</td>\n","      <td>gru</td>\n","      <td>0.002984</td>\n","      <td>0.000012</td>\n","      <td>0.003439</td>\n","      <td>0.002819</td>\n","      <td>153.560272</td>\n","      <td>31333.800781</td>\n","      <td>177.013565</td>\n","      <td>0.001501</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df7245e6-a3b8-4465-b661-34d697ce3d34')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-df7245e6-a3b8-4465-b661-34d697ce3d34 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-df7245e6-a3b8-4465-b661-34d697ce3d34');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":127}]},{"cell_type":"code","source":["df_json_lstm = df_json[df_json['model'] == 'lstm']\n","df_json_lstm[df_json_lstm['MAE_normalize'] == df_json_lstm['MAE_normalize'].min()]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"_Sghviv_lb7R","executionInfo":{"status":"ok","timestamp":1663721304662,"user_tz":-420,"elapsed":191,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}},"outputId":"22aafd0f-48e4-44e2-e35c-b1dd2a1066c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     AHB_number Multi_Uni  validate_size  test_size  hidden_dim  layer_dim  \\\n","759           3     Multi              1        0.1         128          1   \n","\n","     dropout  n_epochs  last_epoch model  MAE_normalize  MSE_normalize  \\\n","759      0.2      1000          16  lstm       0.002838       0.000015   \n","\n","     RMSE_normalize  MAPE_normalize         MAE           MSE        RMSE  \\\n","759        0.003904         0.00268  103.029015  20087.087891  141.728928   \n","\n","         MAPE  \n","759  0.001512  "],"text/html":["\n","  <div id=\"df-d4905278-beea-4596-9cc1-fea8618a8bc7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>AHB_number</th>\n","      <th>Multi_Uni</th>\n","      <th>validate_size</th>\n","      <th>test_size</th>\n","      <th>hidden_dim</th>\n","      <th>layer_dim</th>\n","      <th>dropout</th>\n","      <th>n_epochs</th>\n","      <th>last_epoch</th>\n","      <th>model</th>\n","      <th>MAE_normalize</th>\n","      <th>MSE_normalize</th>\n","      <th>RMSE_normalize</th>\n","      <th>MAPE_normalize</th>\n","      <th>MAE</th>\n","      <th>MSE</th>\n","      <th>RMSE</th>\n","      <th>MAPE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>759</th>\n","      <td>3</td>\n","      <td>Multi</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>128</td>\n","      <td>1</td>\n","      <td>0.2</td>\n","      <td>1000</td>\n","      <td>16</td>\n","      <td>lstm</td>\n","      <td>0.002838</td>\n","      <td>0.000015</td>\n","      <td>0.003904</td>\n","      <td>0.00268</td>\n","      <td>103.029015</td>\n","      <td>20087.087891</td>\n","      <td>141.728928</td>\n","      <td>0.001512</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4905278-beea-4596-9cc1-fea8618a8bc7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d4905278-beea-4596-9cc1-fea8618a8bc7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d4905278-beea-4596-9cc1-fea8618a8bc7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":128}]},{"cell_type":"code","source":["df_json_rnn = df_json[df_json['model'] == 'rnn']\n","df_json_rnn[df_json_rnn['MAE_normalize'] == df_json_rnn['MAE_normalize'].min()]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"g3OWSlwqlcVb","executionInfo":{"status":"ok","timestamp":1663721304664,"user_tz":-420,"elapsed":191,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}},"outputId":"936d404f-5209-482c-d2e1-bb3631815488"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      AHB_number Multi_Uni  validate_size  test_size  hidden_dim  layer_dim  \\\n","2608          10       Uni              1        0.1          64          1   \n","\n","      dropout  n_epochs  last_epoch model  MAE_normalize  MSE_normalize  \\\n","2608      0.5      1000          13   rnn        0.00276       0.000011   \n","\n","      RMSE_normalize  MAPE_normalize         MAE           MSE        RMSE  \\\n","2608        0.003373        0.002613  142.059158  30137.537109  173.601669   \n","\n","         MAPE  \n","2608  0.00139  "],"text/html":["\n","  <div id=\"df-998b31a8-7572-4def-ab25-fd34e12e15b7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>AHB_number</th>\n","      <th>Multi_Uni</th>\n","      <th>validate_size</th>\n","      <th>test_size</th>\n","      <th>hidden_dim</th>\n","      <th>layer_dim</th>\n","      <th>dropout</th>\n","      <th>n_epochs</th>\n","      <th>last_epoch</th>\n","      <th>model</th>\n","      <th>MAE_normalize</th>\n","      <th>MSE_normalize</th>\n","      <th>RMSE_normalize</th>\n","      <th>MAPE_normalize</th>\n","      <th>MAE</th>\n","      <th>MSE</th>\n","      <th>RMSE</th>\n","      <th>MAPE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2608</th>\n","      <td>10</td>\n","      <td>Uni</td>\n","      <td>1</td>\n","      <td>0.1</td>\n","      <td>64</td>\n","      <td>1</td>\n","      <td>0.5</td>\n","      <td>1000</td>\n","      <td>13</td>\n","      <td>rnn</td>\n","      <td>0.00276</td>\n","      <td>0.000011</td>\n","      <td>0.003373</td>\n","      <td>0.002613</td>\n","      <td>142.059158</td>\n","      <td>30137.537109</td>\n","      <td>173.601669</td>\n","      <td>0.00139</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-998b31a8-7572-4def-ab25-fd34e12e15b7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-998b31a8-7572-4def-ab25-fd34e12e15b7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-998b31a8-7572-4def-ab25-fd34e12e15b7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":129}]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[],"background_execution":"on","mount_file_id":"1W39mM01YfW5mZauJqvkQ8tuYHihahIEW","authorship_tag":"ABX9TyPMEXQIzddO7FanmKr2+TVk"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}