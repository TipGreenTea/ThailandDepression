{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1KDRJXuDMNj","outputId":"bae18e01-542a-4cb0-8e4f-a0eb2baa8957","executionInfo":{"status":"ok","timestamp":1665031066714,"user_tz":-420,"elapsed":1919,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1yEjicXasMGFaALQobQWgsSxQgLEtxruC\n","To: /content/ThaiDepression.zip\n","\r  0% 0.00/41.1k [00:00<?, ?B/s]\r100% 41.1k/41.1k [00:00<00:00, 32.7MB/s]\n","Archive:  /content/ThaiDepression.zip\n","  inflating: ThaiDepression/AHB_01.csv  \n","  inflating: ThaiDepression/AHB_02.csv  \n","  inflating: ThaiDepression/AHB_03.csv  \n","  inflating: ThaiDepression/AHB_04.csv  \n","  inflating: ThaiDepression/AHB_05.csv  \n","  inflating: ThaiDepression/AHB_06.csv  \n","  inflating: ThaiDepression/AHB_07.csv  \n","  inflating: ThaiDepression/AHB_08.csv  \n","  inflating: ThaiDepression/AHB_09.csv  \n","  inflating: ThaiDepression/AHB_10.csv  \n","  inflating: ThaiDepression/AHB_11.csv  \n","  inflating: ThaiDepression/AHB_12.csv  \n","  inflating: ThaiDepression/AHB_13.csv  \n","  inflating: ThaiDepression/All_AHB.csv  \n"]}],"source":["! gdown 1yEjicXasMGFaALQobQWgsSxQgLEtxruC\n","! unzip /content/ThaiDepression.zip"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"yu9Iuu5XFvJw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665031071556,"user_tz":-420,"elapsed":4845,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}},"outputId":"0e6e646c-d10a-46cf-fb99-235c930531f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 11348249646624951180\n","xla_global_id: -1\n","]\n"]}],"source":["from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"XY5mr2mxBKNI","executionInfo":{"status":"ok","timestamp":1665031072286,"user_tz":-420,"elapsed":735,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from numpy import array\n","from math import sqrt\n","from numpy import mean\n","from pandas import DataFrame\n","from pandas import concat\n","from pandas import read_csv\n","from keras.callbacks import EarlyStopping\n","from keras.models import Sequential\n","from keras.layers import Flatten\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_absolute_percentage_error\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n","from keras.preprocessing import sequence\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GRU, BatchNormalization\n","from pandas import read_csv\n","from keras import Model\n","from keras.layers import Layer\n","import keras.backend as K\n","from keras.layers import Input, SimpleRNN\n","import json\n","import psutil \n","from datetime import datetime, timezone, timedelta"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"0xd-eM3VJVz5","executionInfo":{"status":"ok","timestamp":1665031072287,"user_tz":-420,"elapsed":19,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}}},"outputs":[],"source":["file_list = os.listdir('ThaiDepression')\n","file_list.sort()\n","df_list = []\n","for i in range(14):\n","    df_list.append(pd.read_csv('ThaiDepression/' + file_list[i], index_col=0))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"kMC4i-X-OTDb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665031072288,"user_tz":-420,"elapsed":20,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}},"outputId":"9ed8dd4f-3cd5-4631-f709-04298d56c984"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[         Chiang_Rai    Nan  Phayao  Phrae  Chiang_Mai  Mae_Hong_Son  Lampang  \\\n"," 2016-10       10383   5315    4266   4057       15408          2002     9011   \n"," 2016-11       10383   5315    4265   4057       15406          2002     9011   \n"," 2016-12       10382   5315    4265   4057       15404          2002     9011   \n"," 2017-01       10382   5655    4265   4057       15393          2002     9011   \n"," 2017-02       10382   5655    4264   4057       15392          2002     9011   \n"," ...             ...    ...     ...    ...         ...           ...      ...   \n"," 2022-03       25223   9711   10788   7988       47292          6924    17444   \n"," 2022-04       25456   9781   10932   8095       48020          7004    17606   \n"," 2022-05       25619   9833   10977   8122       48155          7026    17703   \n"," 2022-06       25836   9884   11069   8161       48616          7090    17783   \n"," 2022-07       26174  10019   11163   8250       49096          7169    17951   \n"," \n","          Lamphun   AHB_1  Country_Level  \n"," 2016-10     3214   53656       607299.0  \n"," 2016-11     3214   53653       611508.0  \n"," 2016-12     3214   53650       614080.0  \n"," 2017-01     3213   53978       620732.0  \n"," 2017-02     3213   53976       627688.0  \n"," ...          ...     ...            ...  \n"," 2022-03    12499  137869      1213727.0  \n"," 2022-04    12650  139544      1225461.0  \n"," 2022-05    12717  140152      1231355.0  \n"," 2022-06    12807  141246      1239786.0  \n"," 2022-07    12910  142732      1250291.0  \n"," \n"," [70 rows x 10 columns],\n","            Tak  Phitsanulok  Phetchabun  Sukhothai  Uttaradit  AHB_2  \\\n"," 2016-10   2558         7875        8783       7396       5982  32594   \n"," 2016-11   2558         7895        8782       7484       5982  32701   \n"," 2016-12   2558         7895        8781       7521       5982  32737   \n"," 2017-01   2558         7892        8780       7605       6335  33170   \n"," 2017-02   2558         7946        8777       7624       6492  33397   \n"," ...        ...          ...         ...        ...        ...    ...   \n"," 2022-03  10165        18907       15455      12558       9036  66121   \n"," 2022-04  10318        19057       15632      12662       9152  66821   \n"," 2022-05  10320        19148       15671      12703       9171  67013   \n"," 2022-06  10323        19298       15743      12779       9256  67399   \n"," 2022-07  10547        19530       15811      12869       9385  68142   \n"," \n","          Country_Level  \n"," 2016-10       607299.0  \n"," 2016-11       611508.0  \n"," 2016-12       614080.0  \n"," 2017-01       620732.0  \n"," 2017-02       627688.0  \n"," ...                ...  \n"," 2022-03      1213727.0  \n"," 2022-04      1225461.0  \n"," 2022-05      1231355.0  \n"," 2022-06      1239786.0  \n"," 2022-07      1250291.0  \n"," \n"," [70 rows x 7 columns],\n","          Chai_Nat  Kamphaeng_Phet  Phichit  Nakhon_Sawan  Uthai_Thani  AHB_3  \\\n"," 2016-10      3432            6997     5336         10476         3354  29595   \n"," 2016-11      3439            6997     5336         10568         3354  29694   \n"," 2016-12      3442            6996     5336         10568         3354  29696   \n"," 2017-01      3442            6993     5336         10671         3354  29796   \n"," 2017-02      3442            6993     5335         10685         3354  29809   \n"," ...           ...             ...      ...           ...          ...    ...   \n"," 2022-03      7826           16104     9734         25026         8740  67430   \n"," 2022-04      7886           16315     9822         25322         8785  68130   \n"," 2022-05      7911           16374     9845         25359         8801  68290   \n"," 2022-06      7954           16448     9904         25557         8855  68718   \n"," 2022-07      8008           16513     9976         25736         8907  69140   \n"," \n","          Country_Level  \n"," 2016-10       607299.0  \n"," 2016-11       611508.0  \n"," 2016-12       614080.0  \n"," 2017-01       620732.0  \n"," 2017-02       627688.0  \n"," ...                ...  \n"," 2022-03      1213727.0  \n"," 2022-04      1225461.0  \n"," 2022-05      1231355.0  \n"," 2022-06      1239786.0  \n"," 2022-07      1250291.0  \n"," \n"," [70 rows x 7 columns],\n","          Nonthaburi  Pathum_Thani  Phra_Nakhon Si_Ayutthaya  Saraburi  \\\n"," 2016-10        9731          8862                     10474      6518   \n"," 2016-11        9745          8870                     10474      6518   \n"," 2016-12        9751          9748                     10346      6517   \n"," 2017-01        9765          9847                     10916      6515   \n"," 2017-02        9758         10183                     10913      6513   \n"," ...             ...           ...                       ...       ...   \n"," 2022-03       34826         17831                     16303     11156   \n"," 2022-04       35225         18031                     16414     11327   \n"," 2022-05       35361         18051                     16454     11373   \n"," 2022-06       35606         18176                     16573     11464   \n"," 2022-07       35923         18319                     16695     11551   \n"," \n","          Lopburi  Sing_Buri  Ang_Thong  Nakhon_Nayok   AHB_4  Country_Level  \n"," 2016-10     7734       3388       5583          3312   55602       607299.0  \n"," 2016-11     7734       3388       5583          3312   55624       611508.0  \n"," 2016-12     7732       3388       5583          3311   56376       614080.0  \n"," 2017-01     7714       3388       5686          3310   57141       620732.0  \n"," 2017-02     7739       3388       5697          3310   57501       627688.0  \n"," ...          ...        ...        ...           ...     ...            ...  \n"," 2022-03    13945       8854       8916          4820  116651      1213727.0  \n"," 2022-04    14061       8975       8973          4868  117874      1225461.0  \n"," 2022-05    14140       9016       8992          4884  118271      1231355.0  \n"," 2022-06    14230       9094       9040          4923  119106      1239786.0  \n"," 2022-07    14332       9204       9119          4963  120106      1250291.0  \n"," \n"," [70 rows x 10 columns],\n","          Kanchanaburi  Nakhon_Pathom  Ratchaburi  Suphan_Buri  \\\n"," 2016-10          8067           7603        6324         5171   \n"," 2016-11          8238           7603        6494         5231   \n"," 2016-12          8237           7604        6519         5230   \n"," 2017-01          8276           7858        6640         5305   \n"," 2017-02          8276           7872        6766         5330   \n"," ...               ...            ...         ...          ...   \n"," 2022-03         14407          11406       12419        12907   \n"," 2022-04         14536          11443       12626        13047   \n"," 2022-05         14583          11462       12685        13079   \n"," 2022-06         14699          11508       12815        13179   \n"," 2022-07         14826          11564       13027        13307   \n"," \n","          Prachuap_Khiri_Khan  Phetchaburi  Samut_Songkhram  Samut_Sakhon  \\\n"," 2016-10                 6132         4589             1507          3651   \n"," 2016-11                 6198         4633             1522          3772   \n"," 2016-12                 6197         4668             1522          3770   \n"," 2017-01                 6319         4696             1720          3837   \n"," 2017-02                 6622         4731             1725          3947   \n"," ...                      ...          ...              ...           ...   \n"," 2022-03                10347         8806             2945         10015   \n"," 2022-04                10473         8878             3128         10151   \n"," 2022-05                10489         8899             3128         10209   \n"," 2022-06                10575         8947             2945         10340   \n"," 2022-07                10706         9017             3152         10458   \n"," \n","          AHB_5  Country_Level  \n"," 2016-10  43044       607299.0  \n"," 2016-11  43691       611508.0  \n"," 2016-12  43747       614080.0  \n"," 2017-01  44651       620732.0  \n"," 2017-02  45269       627688.0  \n"," ...        ...            ...  \n"," 2022-03  83252      1213727.0  \n"," 2022-04  84282      1225461.0  \n"," 2022-05  84534      1231355.0  \n"," 2022-06  85008      1239786.0  \n"," 2022-07  86057      1250291.0  \n"," \n"," [70 rows x 10 columns],\n","          Chachoengsao  Prachinburi  Sa_Kaeo  Samut_Prakan  Chanthaburi  \\\n"," 2016-10          6456         4271     6913          6004         2572   \n"," 2016-11          6454         4271     6911          6004         2572   \n"," 2016-12          6455         4271     6910          6002         2572   \n"," 2017-01          6456         4271     6910          6000         3356   \n"," 2017-02          6456         4270     6908          6034         3354   \n"," ...               ...          ...      ...           ...          ...   \n"," 2022-03         17553         6847     9893         13608         7103   \n"," 2022-04         17705         6902     9946         13773         7231   \n"," 2022-05         17723         6917     9965         13846         7270   \n"," 2022-06         17801         6959    10008         13953         7372   \n"," 2022-07         17948         7018    10075         14112         7500   \n"," \n","          Chonburi  Trat  Rayong  AHB_6  Country_Level  \n"," 2016-10      7908  1265    3357  38746       607299.0  \n"," 2016-11      7908  1312    3357  38789       611508.0  \n"," 2016-12      7908  1312    3357  38787       614080.0  \n"," 2017-01      8408  1318    3356  40075       620732.0  \n"," 2017-02      8406  1325    3355  40108       627688.0  \n"," ...           ...   ...     ...    ...            ...  \n"," 2022-03     18653  3504    7466  84627      1213727.0  \n"," 2022-04     18936  3541    7593  85627      1225461.0  \n"," 2022-05     19009  3561    7644  85935      1231355.0  \n"," 2022-06     19222  3591    7738  86644      1239786.0  \n"," 2022-07     19514  3625    7837  87629      1250291.0  \n"," \n"," [70 rows x 10 columns],\n","          Kalasin  Khon_Kaen  Maha_Sarakham  Roi_Et   AHB_7  Country_Level\n"," 2016-10     8477      19045          14203   12712   54437       607299.0\n"," 2016-11     8520      19234          14228   12794   54776       611508.0\n"," 2016-12     8532      19470          14233   12852   55087       614080.0\n"," 2017-01     8689      19816          14231   12925   55661       620732.0\n"," 2017-02     8806      20064          14231   12944   56045       627688.0\n"," ...          ...        ...            ...     ...     ...            ...\n"," 2022-03    16722      36856          22628   23368   99574      1213727.0\n"," 2022-04    16809      37331          22886   23491  100517      1225461.0\n"," 2022-05    16862      37389          22928   23547  100726      1231355.0\n"," 2022-06    16962      37627          23028   23652  101269      1239786.0\n"," 2022-07    17039      37917          23217   23828  102001      1250291.0\n"," \n"," [70 rows x 6 columns],\n","          Bueng_Kan   Loei  Nong_Khai  Nong_Bua_Lamphu  Udon_Thani  \\\n"," 2016-10       6450  10454       3474             6155       16452   \n"," 2016-11       6453  10454       3474             6155       16452   \n"," 2016-12       6453  10454       3474             6155       16451   \n"," 2017-01       6578  10476       3479             6156       16454   \n"," 2017-02       6576  10473       3565             6156       16498   \n"," ...            ...    ...        ...              ...         ...   \n"," 2022-03       7628  14085      10383             8924       26313   \n"," 2022-04       7663  14184      10457             8974       26313   \n"," 2022-05       7681  14204      10491             9004       26921   \n"," 2022-06       7711  14281      10544             9043       27021   \n"," 2022-07       7742  14362      10623             9070       27181   \n"," \n","          Nakhon_Phanom  Sakon_Nakhon   AHB_8  Country_Level  \n"," 2016-10           8279         14756   66020       607299.0  \n"," 2016-11           8281         14805   66074       611508.0  \n"," 2016-12           8575         14830   66392       614080.0  \n"," 2017-01           8581         14940   66664       620732.0  \n"," 2017-02           8581         14960   66809       627688.0  \n"," ...                ...           ...     ...            ...  \n"," 2022-03          11933         22756  102022      1213727.0  \n"," 2022-04          11988         22882  102461      1225461.0  \n"," 2022-05          11998         22920  103219      1231355.0  \n"," 2022-06          12042         23038  103680      1239786.0  \n"," 2022-07          12146         23272  104396      1250291.0  \n"," \n"," [70 rows x 9 columns],\n","          Chaiyaphum  Nakhon_Ratchasima  Buriram  Surin   AHB_9  Country_Level\n"," 2016-10       11333              28200    17408  14584   71525       607299.0\n"," 2016-11       11504              29177    17617  14599   72897       611508.0\n"," 2016-12       11526              29236    17745  14670   73177       614080.0\n"," 2017-01       11553              29379    17970  14837   73739       620732.0\n"," 2017-02       14161              30229    18145  15324   77859       627688.0\n"," ...             ...                ...      ...    ...     ...            ...\n"," 2022-03       23217              61826    33541  25809  144393      1213727.0\n"," 2022-04       23427              62345    33727  26191  145690      1225461.0\n"," 2022-05       23533              62562    33815  26899  146809      1231355.0\n"," 2022-06       23689              63020    33994  27467  148170      1239786.0\n"," 2022-07       23890              63468    34396  27841  149595      1250291.0\n"," \n"," [70 rows x 6 columns],\n","          Mukdahan  Yasothon  Sisaket  Ubon_Ratchathani  Amnat_Charoen  AHB_10  \\\n"," 2016-10      4919      6937    11902             20439           2494   46691   \n"," 2016-11      5881      6937    12045             20438           2504   47805   \n"," 2016-12      5890      6937    12379             20436           2510   48152   \n"," 2017-01      6015      6937    12640             20598           2514   48704   \n"," 2017-02      6015      6937    13100             20657           2538   49247   \n"," ...           ...       ...      ...               ...            ...     ...   \n"," 2022-03      9980     12847    26734             45225           6692  101478   \n"," 2022-04     10036     12906    26872             45617           6745  102176   \n"," 2022-05     10053     12935    26940             45725           6776  102429   \n"," 2022-06     10101     12973    27100             45861           6815  102850   \n"," 2022-07     10182     13046    27298             46184           6878  103588   \n"," \n","          Country_Level  \n"," 2016-10       607299.0  \n"," 2016-11       611508.0  \n"," 2016-12       614080.0  \n"," 2017-01       620732.0  \n"," 2017-02       627688.0  \n"," ...                ...  \n"," 2022-03      1213727.0  \n"," 2022-04      1225461.0  \n"," 2022-05      1231355.0  \n"," 2022-06      1239786.0  \n"," 2022-07      1250291.0  \n"," \n"," [70 rows x 7 columns],\n","          Chumphon  Nakhon_Si_Thammarat  Surat_Thani  Krabi  Phang_Nga  Phuket  \\\n"," 2016-10      3775                13009        13674   2705       2870    2036   \n"," 2016-11      3777                13049        13684   2715       2870    2035   \n"," 2016-12      3778                13184        13681   2716       2870    2035   \n"," 2017-01      3804                13338        13681   2802       2868    2033   \n"," 2017-02      3816                13420        13681   2852       2868    2043   \n"," ...           ...                  ...          ...    ...        ...     ...   \n"," 2022-03      6551                26018        26693   5683       4434    4758   \n"," 2022-04      6595                26198        26985   5744       4464    4887   \n"," 2022-05      6615                26268        27067   5769       4479    4896   \n"," 2022-06      6723                26478        27192   5823       4496    4944   \n"," 2022-07      6779                26784        27389   5924       4537    5024   \n"," \n","          Ranong  AHB_11  Country_Level  \n"," 2016-10    1447   39516       607299.0  \n"," 2016-11    1457   39587       611508.0  \n"," 2016-12    1485   39749       614080.0  \n"," 2017-01    1536   40062       620732.0  \n"," 2017-02    1543   40223       627688.0  \n"," ...         ...     ...            ...  \n"," 2022-03    3835   77972      1213727.0  \n"," 2022-04    3894   78767      1225461.0  \n"," 2022-05    3901   78995      1231355.0  \n"," 2022-06    3940   79596      1239786.0  \n"," 2022-07    3976   80413      1250291.0  \n"," \n"," [70 rows x 9 columns],\n","          Phatthalung  Trang  Narathiwat  Pattani  Yala  Songkhla  Satun  \\\n"," 2016-10         6950   9535        9569     6626  3597     14123   2229   \n"," 2016-11         6959   9626        9608     6684  3598     14231   2260   \n"," 2016-12         6970   9682        9683     6735  3598     14330   2281   \n"," 2017-01         6998   9718        9713     6766  3738     14637   2278   \n"," 2017-02         7006   9934        9726     6804  3738     14682   2324   \n"," ...              ...    ...         ...      ...   ...       ...    ...   \n"," 2022-03        11218  14532       14098    11015  7550     29019   5293   \n"," 2022-04        11315  14652       14205    11083  7633     29390   5361   \n"," 2022-05        11360  14699       14262    11126  7662     29519   5391   \n"," 2022-06        11444  14770       14353    11199  7736     29832   5442   \n"," 2022-07        11562  14892       14479    11267  7786     30126   5510   \n"," \n","          AHB_12  Country_Level  \n"," 2016-10   52629       607299.0  \n"," 2016-11   52966       611508.0  \n"," 2016-12   53279       614080.0  \n"," 2017-01   53848       620732.0  \n"," 2017-02   54214       627688.0  \n"," ...         ...            ...  \n"," 2022-03   92725      1213727.0  \n"," 2022-04   93639      1225461.0  \n"," 2022-05   94019      1231355.0  \n"," 2022-06   94776      1239786.0  \n"," 2022-07   95622      1250291.0  \n"," \n"," [70 rows x 9 columns],\n","          Bangkok  AHB_13  Country_Level\n"," 2016-10    23244   23244       607299.0\n"," 2016-11    23251   23251       611508.0\n"," 2016-12    23251   23251       614080.0\n"," 2017-01    23243   23243       620732.0\n"," 2017-02    23231   23231       627688.0\n"," ...          ...     ...            ...\n"," 2022-03    39613   39613      1213727.0\n"," 2022-04    39933   39933      1225461.0\n"," 2022-05    40963   40963      1231355.0\n"," 2022-06    41324   41324      1239786.0\n"," 2022-07    40870   40870      1250291.0\n"," \n"," [70 rows x 3 columns],\n","          Chiang_Rai    Nan  Phayao  Phrae  Chiang_Mai  Mae_Hong_Son  Lampang  \\\n"," 2016-10       10383   5315    4266   4057       15408          2002     9011   \n"," 2016-11       10383   5315    4265   4057       15406          2002     9011   \n"," 2016-12       10382   5315    4265   4057       15404          2002     9011   \n"," 2017-01       10382   5655    4265   4057       15393          2002     9011   \n"," 2017-02       10382   5655    4264   4057       15392          2002     9011   \n"," ...             ...    ...     ...    ...         ...           ...      ...   \n"," 2022-03       25223   9711   10788   7988       47292          6924    17444   \n"," 2022-04       25456   9781   10932   8095       48020          7004    17606   \n"," 2022-05       25619   9833   10977   8122       48155          7026    17703   \n"," 2022-06       25836   9884   11069   8161       48616          7090    17783   \n"," 2022-07       26174  10019   11163   8250       49096          7169    17951   \n"," \n","          Lamphun    Tak  Phitsanulok  ...  AHB_5  AHB_6   AHB_7   AHB_8  \\\n"," 2016-10     3214   2558         7875  ...  43044  38746   54437   66020   \n"," 2016-11     3214   2558         7895  ...  43691  38789   54776   66074   \n"," 2016-12     3214   2558         7895  ...  43747  38787   55087   66392   \n"," 2017-01     3213   2558         7892  ...  44651  40075   55661   66664   \n"," 2017-02     3213   2558         7946  ...  45269  40108   56045   66809   \n"," ...          ...    ...          ...  ...    ...    ...     ...     ...   \n"," 2022-03    12499  10165        18907  ...  83252  84627   99574  102022   \n"," 2022-04    12650  10318        19057  ...  84282  85627  100517  102461   \n"," 2022-05    12717  10320        19148  ...  84534  85935  100726  103219   \n"," 2022-06    12807  10323        19298  ...  85008  86644  101269  103680   \n"," 2022-07    12910  10547        19530  ...  86057  87629  102001  104396   \n"," \n","           AHB_9  AHB_10  AHB_11  AHB_12  AHB_13  Country_Level  \n"," 2016-10   71525   46691   39516   52629   23244       607299.0  \n"," 2016-11   72897   47805   39587   52966   23251       611508.0  \n"," 2016-12   73177   48152   39749   53279   23251       614080.0  \n"," 2017-01   73739   48704   40062   53848   23243       620732.0  \n"," 2017-02   77859   49247   40223   54214   23231       627688.0  \n"," ...         ...     ...     ...     ...     ...            ...  \n"," 2022-03  144393  101478   77972   92725   39613      1213727.0  \n"," 2022-04  145690  102176   78767   93639   39933      1225461.0  \n"," 2022-05  146809  102429   78995   94019   40963      1231355.0  \n"," 2022-06  148170  102850   79596   94776   41324      1239786.0  \n"," 2022-07  149595  103588   80413   95622   40870      1250291.0  \n"," \n"," [70 rows x 91 columns]]"]},"metadata":{},"execution_count":5}],"source":["df_list"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"SU_ny31AUe6O","executionInfo":{"status":"ok","timestamp":1665031072289,"user_tz":-420,"elapsed":19,"user":{"displayName":"Pasinpat Vitoochuleechoti","userId":"04423538674438574487"}}},"outputs":[],"source":["# split a univariate sequence into samples\n","def uni_split_sequence(sequence, n_steps):\n","    X, y = list(), list()\n","    for i in range(len(sequence)):\n","        # find the end of this pattern\n","        end_ix = i + n_steps\n","        # check if we are beyond the sequence\n","        if end_ix > len(sequence)-1:\n","            break\n","        # gather input and output parts of the pattern\n","        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","    return array(X), array(y)\n","\n","\n","    # split a multivariate sequence into samples\n","def multi_split_sequences(sequences, n_steps):\n","    X, y = list(), list()\n","    for i in range(len(sequences)):\n","        # find the end of this pattern\n","        end_ix = i + n_steps\n","        # check if we are beyond the dataset\n","        if end_ix > len(sequences):\n","            break\n","        # gather input and output parts of the pattern\n","        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","    return array(X), array(y)\n","\n","\n","def get_scaler(scaler):\n","    scalers = {\n","        \"minmax\": MinMaxScaler,\n","        \"standard\": StandardScaler,\n","        \"maxabs\": MaxAbsScaler,\n","        \"robust\": RobustScaler,\n","    }\n","    return scalers.get(scaler.lower())()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3pQoCYDv5gjk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d91033fd-61af-4dce-a6b6-673cfacabf43"},"outputs":[{"output_type":"stream","name":"stdout","text":["PROGRESS: [ 1 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 2,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 20,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.1269855370637494,\n","  \"MSE_normalize\": 0.016179131977611432,\n","  \"RMSE_normalize\": 0.12719721686267915,\n","  \"MAPE_normalize\": 0.1316971634400816,\n","  \"MAE\": 6039.940848214282,\n","  \"MSE\": 36602612.29008261,\n","  \"RMSE\": 6050.009280164999,\n","  \"MAPE\": 0.060210251053874696\n","}\n","Percent CPU Usage: 5.0\n","Percent Ram Usage: 11.4\n","2022-10-06 11:37:58.537217+07:00\n","PROGRESS: [ 2 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 2,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 12,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.08309559727874574,\n","  \"MSE_normalize\": 0.006944544726888108,\n","  \"RMSE_normalize\": 0.08333393502582312,\n","  \"MAPE_normalize\": 0.08614131319533129,\n","  \"MAE\": 3952.35602678571,\n","  \"MSE\": 15710856.621015247,\n","  \"RMSE\": 3963.692296459861,\n","  \"MAPE\": 0.039391998159892014\n","}\n","Percent CPU Usage: 56.6\n","Percent Ram Usage: 11.7\n","2022-10-06 11:38:04.849269+07:00\n","PROGRESS: [ 3 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 4,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 14,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.2344547466666503,\n","  \"MSE_normalize\": 0.05516538020345257,\n","  \"RMSE_normalize\": 0.2348731151142092,\n","  \"MAPE_normalize\": 0.2431220248453909,\n","  \"MAE\": 11151.60491071428,\n","  \"MSE\": 124802505.5495779,\n","  \"RMSE\": 11171.50417578483,\n","  \"MAPE\": 0.11116019642953999\n","}\n","Percent CPU Usage: 56.5\n","Percent Ram Usage: 11.7\n","2022-10-06 11:38:10.693511+07:00\n","PROGRESS: [ 4 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 4,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 24,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.06974929136345819,\n","  \"MSE_normalize\": 0.0049299831959417366,\n","  \"RMSE_normalize\": 0.0702138390628353,\n","  \"MAPE_normalize\": 0.07223331617265731,\n","  \"MAE\": 3317.55915178571,\n","  \"MSE\": 11153296.014848959,\n","  \"RMSE\": 3339.6550742328104,\n","  \"MAPE\": 0.03304998332983127\n","}\n","Percent CPU Usage: 61.1\n","Percent Ram Usage: 11.8\n","2022-10-06 11:38:17.306680+07:00\n","PROGRESS: [ 5 / 5040 ]\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f72e80f65f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 8,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 18,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.20820000758649373,\n","  \"MSE_normalize\": 0.04350538545474684,\n","  \"RMSE_normalize\": 0.20857944638613565,\n","  \"MAPE_normalize\": 0.2158945411736608,\n","  \"MAE\": 9902.82366071428,\n","  \"MSE\": 98423687.7232491,\n","  \"RMSE\": 9920.871318752657,\n","  \"MAPE\": 0.09871177762641403\n","}\n","Percent CPU Usage: 54.0\n","Percent Ram Usage: 12.2\n","2022-10-06 11:38:22.997780+07:00\n","PROGRESS: [ 6 / 5040 ]\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f72e845c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 8,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 15,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.13259279293129875,\n","  \"MSE_normalize\": 0.01771472994028401,\n","  \"RMSE_normalize\": 0.13309669394948925,\n","  \"MAPE_normalize\": 0.13740259888361722,\n","  \"MAE\": 6306.642857142853,\n","  \"MSE\": 40076628.26161407,\n","  \"RMSE\": 6330.610417772845,\n","  \"MAPE\": 0.06284599543362088\n","}\n","Percent CPU Usage: 63.1\n","Percent Ram Usage: 12.2\n","2022-10-06 11:38:30.526471+07:00\n","PROGRESS: [ 7 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 16,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 18,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.16874670547554924,\n","  \"MSE_normalize\": 0.028584742776323325,\n","  \"RMSE_normalize\": 0.16907023030777277,\n","  \"MAPE_normalize\": 0.1749792872587494,\n","  \"MAE\": 8026.268973214282,\n","  \"MSE\": 64668250.06744378,\n","  \"RMSE\": 8041.657171718015,\n","  \"MAPE\": 0.08000538436884143\n","}\n","Percent CPU Usage: 58.0\n","Percent Ram Usage: 12.2\n","2022-10-06 11:38:35.613999+07:00\n","PROGRESS: [ 8 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 16,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 25,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.09731309319701967,\n","  \"MSE_normalize\": 0.009567149130750957,\n","  \"RMSE_normalize\": 0.09781180465951417,\n","  \"MAPE_normalize\": 0.10080911936484069,\n","  \"MAE\": 4628.59821428571,\n","  \"MSE\": 21644069.776803117,\n","  \"RMSE\": 4652.318752708494,\n","  \"MAPE\": 0.04611707520784967\n","}\n","Percent CPU Usage: 52.9\n","Percent Ram Usage: 12.0\n","2022-10-06 11:38:42.096179+07:00\n","PROGRESS: [ 9 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 32,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 41,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.5488649009752419,\n","  \"MSE_normalize\": 0.301605552083358,\n","  \"RMSE_normalize\": 0.5491862635603316,\n","  \"MAPE_normalize\": 0.5695057683946356,\n","  \"MAE\": 26106.210937499996,\n","  \"MSE\": 682332566.996015,\n","  \"RMSE\": 26121.49626258065,\n","  \"MAPE\": 0.2603022586042664\n","}\n","Percent CPU Usage: 77.9\n","Percent Ram Usage: 12.0\n","2022-10-06 11:38:49.149938+07:00\n","PROGRESS: [ 10 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 32,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 34,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.12590781696661632,\n","  \"MSE_normalize\": 0.015949361538753863,\n","  \"RMSE_normalize\": 0.12629078168557617,\n","  \"MAPE_normalize\": 0.13050511124859174,\n","  \"MAE\": 5988.677455357139,\n","  \"MSE\": 36082760.90885703,\n","  \"RMSE\": 6006.892783199733,\n","  \"MAPE\": 0.05968372016022166\n","}\n","Percent CPU Usage: 57.8\n","Percent Ram Usage: 12.0\n","2022-10-06 11:38:54.340168+07:00\n","PROGRESS: [ 11 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 64,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 14,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.7598617778451249,\n","  \"MSE_normalize\": 0.5777961118025894,\n","  \"RMSE_normalize\": 0.760129009973037,\n","  \"MAPE_normalize\": 0.7885853109443639,\n","  \"MAE\": 36142.06640624999,\n","  \"MSE\": 1307167903.2263944,\n","  \"RMSE\": 36154.7770457293,\n","  \"MAPE\": 0.3603995729030111\n","}\n","Percent CPU Usage: 55.2\n","Percent Ram Usage: 12.1\n","2022-10-06 11:39:00.129506+07:00\n","PROGRESS: [ 12 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 64,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 40,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.26401318387782324,\n","  \"MSE_normalize\": 0.06989123080775674,\n","  \"RMSE_normalize\": 0.26436949674226173,\n","  \"MAPE_normalize\": 0.2738240255423383,\n","  \"MAE\": 12557.523437499996,\n","  \"MSE\": 158117323.26364562,\n","  \"RMSE\": 12574.471092799316,\n","  \"MAPE\": 0.1251851384470886\n","}\n","Percent CPU Usage: 57.8\n","Percent Ram Usage: 11.5\n","2022-10-06 11:39:05.894234+07:00\n","PROGRESS: [ 13 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 128,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 61,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.6750694895587793,\n","  \"MSE_normalize\": 0.4561422337302809,\n","  \"RMSE_normalize\": 0.6753830274224256,\n","  \"MAPE_normalize\": 0.7005200838714408,\n","  \"MAE\": 32109.005580357138,\n","  \"MSE\": 1031946151.1193758,\n","  \"RMSE\": 32123.918676266378,\n","  \"MAPE\": 0.320168773747661\n","}\n","Percent CPU Usage: 57.6\n","Percent Ram Usage: 11.4\n","2022-10-06 11:39:11.953336+07:00\n","PROGRESS: [ 14 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 1,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 128,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 29,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.5950896739053871,\n","  \"MSE_normalize\": 0.3544981390504711,\n","  \"RMSE_normalize\": 0.5953974630870299,\n","  \"MAPE_normalize\": 0.6174999306556136,\n","  \"MAE\": 28304.84598214285,\n","  \"MSE\": 801993265.4181254,\n","  \"RMSE\": 28319.48561358637,\n","  \"MAPE\": 0.28223108937802854\n","}\n","Percent CPU Usage: 64.0\n","Percent Ram Usage: 11.3\n","2022-10-06 11:39:17.947965+07:00\n","PROGRESS: [ 15 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 2,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 17,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.021929310810239886,\n","  \"MSE_normalize\": 0.0004935950989757753,\n","  \"RMSE_normalize\": 0.0222170002245077,\n","  \"MAPE_normalize\": 0.022752201102178564,\n","  \"MAE\": 1043.0435267857101,\n","  \"MSE\": 1116672.5838535768,\n","  \"RMSE\": 1056.7272987169285,\n","  \"MAPE\": 0.010399664982585285\n","}\n","Percent CPU Usage: 54.7\n","Percent Ram Usage: 11.3\n","2022-10-06 11:39:32.738365+07:00\n","PROGRESS: [ 16 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 2,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 18,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.04556685251305535,\n","  \"MSE_normalize\": 0.0020962921599541992,\n","  \"RMSE_normalize\": 0.045785283224571184,\n","  \"MAPE_normalize\": 0.04721831481505684,\n","  \"MAE\": 2167.3426339285675,\n","  \"MSE\": 4742517.379281164,\n","  \"RMSE\": 2177.732164266571,\n","  \"MAPE\": 0.02159734752551568\n","}\n","Percent CPU Usage: 60.1\n","Percent Ram Usage: 11.4\n","2022-10-06 11:39:44.531677+07:00\n","PROGRESS: [ 17 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 4,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 22,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.028513571955150425,\n","  \"MSE_normalize\": 0.0008261606362194409,\n","  \"RMSE_normalize\": 0.028743010214997332,\n","  \"MAPE_normalize\": 0.02960814344554093,\n","  \"MAE\": 1356.218749999996,\n","  \"MSE\": 1869049.735979341,\n","  \"RMSE\": 1367.1319380291504,\n","  \"MAPE\": 0.013527326450530276\n","}\n","Percent CPU Usage: 61.4\n","Percent Ram Usage: 11.4\n","2022-10-06 11:39:57.056884+07:00\n","PROGRESS: [ 18 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 4,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 22,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.0779394378778058,\n","  \"MSE_normalize\": 0.006103530988107806,\n","  \"RMSE_normalize\": 0.07812509832382808,\n","  \"MAPE_normalize\": 0.08080843099196663,\n","  \"MAE\": 3707.109374999996,\n","  \"MSE\": 13808211.54112023,\n","  \"RMSE\": 3715.9401961172935,\n","  \"MAPE\": 0.036950236395105016\n","}\n","Percent CPU Usage: 61.0\n","Percent Ram Usage: 11.4\n","2022-10-06 11:40:09.844168+07:00\n","PROGRESS: [ 19 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 8,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 18,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.2024144146217764,\n","  \"MSE_normalize\": 0.041086180825312554,\n","  \"RMSE_normalize\": 0.20269726398082574,\n","  \"MAPE_normalize\": 0.20993553206063223,\n","  \"MAE\": 9627.639508928567,\n","  \"MSE\": 92950672.61574875,\n","  \"RMSE\": 9641.092916041664,\n","  \"MAPE\": 0.09597716378759316\n","}\n","Percent CPU Usage: 54.8\n","Percent Ram Usage: 11.4\n","2022-10-06 11:40:22.302862+07:00\n","PROGRESS: [ 20 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 8,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 14,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.19275755958354224,\n","  \"MSE_normalize\": 0.03727404255604424,\n","  \"RMSE_normalize\": 0.19306486618762161,\n","  \"MAPE_normalize\": 0.19989971047911773,\n","  \"MAE\": 9168.321428571424,\n","  \"MSE\": 84326351.3965715,\n","  \"RMSE\": 9182.938059062117,\n","  \"MAPE\": 0.09139407757428826\n","}\n","Percent CPU Usage: 58.5\n","Percent Ram Usage: 11.4\n","2022-10-06 11:40:33.704768+07:00\n","PROGRESS: [ 21 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 16,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 19,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.20010567128114792,\n","  \"MSE_normalize\": 0.04020703498224718,\n","  \"RMSE_normalize\": 0.20051691944134586,\n","  \"MAPE_normalize\": 0.20748203336070675,\n","  \"MAE\": 9517.82589285714,\n","  \"MSE\": 90961741.78147663,\n","  \"RMSE\": 9537.386527842762,\n","  \"MAPE\": 0.09487013040008219\n","}\n","Percent CPU Usage: 58.2\n","Percent Ram Usage: 11.4\n","2022-10-06 11:40:44.760984+07:00\n","PROGRESS: [ 22 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 16,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 22,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.14263149235386258,\n","  \"MSE_normalize\": 0.02044868246060257,\n","  \"RMSE_normalize\": 0.14299888971807637,\n","  \"MAPE_normalize\": 0.14786163894367738,\n","  \"MAE\": 6784.122767857139,\n","  \"MSE\": 46261730.409423776,\n","  \"RMSE\": 6801.59763654274,\n","  \"MAPE\": 0.06761586523486106\n","}\n","Percent CPU Usage: 58.5\n","Percent Ram Usage: 11.4\n","2022-10-06 11:40:56.203282+07:00\n","PROGRESS: [ 23 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 32,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 39,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.22258610801493872,\n","  \"MSE_normalize\": 0.04973077253079657,\n","  \"RMSE_normalize\": 0.2230039742488832,\n","  \"MAPE_normalize\": 0.23080627886249014,\n","  \"MAE\": 10587.08370535714,\n","  \"MSE\": 112507581.43341056,\n","  \"RMSE\": 10606.959103975585,\n","  \"MAPE\": 0.1055312546405512\n","}\n","Percent CPU Usage: 57.7\n","Percent Ram Usage: 11.4\n","2022-10-06 11:41:07.461127+07:00\n","PROGRESS: [ 24 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 32,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 28,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.13735245372024354,\n","  \"MSE_normalize\": 0.018975793976235826,\n","  \"RMSE_normalize\": 0.137752655060568,\n","  \"MAPE_normalize\": 0.14237285779220069,\n","  \"MAE\": 6533.032366071425,\n","  \"MSE\": 42929589.3980102,\n","  \"RMSE\": 6552.067566654834,\n","  \"MAPE\": 0.06510991130761483\n","}\n","Percent CPU Usage: 56.9\n","Percent Ram Usage: 11.4\n","2022-10-06 11:41:18.874845+07:00\n","PROGRESS: [ 25 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 64,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 77,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.2926438151884224,\n","  \"MSE_normalize\": 0.08587222652467155,\n","  \"RMSE_normalize\": 0.2930396330271241,\n","  \"MAPE_normalize\": 0.3035184419492808,\n","  \"MAE\": 13919.311383928567,\n","  \"MSE\": 194271692.82473308,\n","  \"RMSE\": 13938.138068792872,\n","  \"MAPE\": 0.13876068696119778\n","}\n","Percent CPU Usage: 52.7\n","Percent Ram Usage: 11.4\n","2022-10-06 11:41:31.601994+07:00\n","PROGRESS: [ 26 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"mul\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 64,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 59,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.157296763908537,\n","  \"MSE_normalize\": 0.024936954591541334,\n","  \"RMSE_normalize\": 0.15791439007114372,\n","  \"MAPE_normalize\": 0.1629960848825697,\n","  \"MAE\": 7481.664062499995,\n","  \"MSE\": 56415734.59433414,\n","  \"RMSE\": 7511.040846269852,\n","  \"MAPE\": 0.0745537677521895\n","}\n","Percent CPU Usage: 70.9\n","Percent Ram Usage: 11.4\n","2022-10-06 11:41:44.021454+07:00\n","PROGRESS: [ 27 / 5040 ]\n","{\n","  \"model\": \"BiLSTM\",\n","  \"AHB_number\": \"AHB_7\",\n","  \"Multi_Uni\": \"uni\",\n","  \"n_layer\": 2,\n","  \"BiLSTM_hidden_units\": 8,\n","  \"dropouts\": 0.2,\n","  \"n_batch\": 128,\n","  \"n_steps\": 2,\n","  \"last_epoch\": 62,\n","  \"patience_list\": 3,\n","  \"MAE_normalize\": 0.23405891222069694,\n","  \"MSE_normalize\": 0.05496825556250474,\n","  \"RMSE_normalize\": 0.23445309885455712,\n","  \"MAPE_normalize\": 0.24272193821923033,\n","  \"MAE\": 11132.77790178571,\n","  \"MSE\": 124356552.05525418,\n","  \"RMSE\": 11151.526893446215,\n","  \"MAPE\": 0.11097469394055129\n","}\n","Percent CPU Usage: 57.9\n","Percent Ram Usage: 11.5\n","2022-10-06 11:41:55.487517+07:00\n","PROGRESS: [ 28 / 5040 ]\n"]}],"source":["%matplotlib inline\n","# define scope of configs\n","AHB_list = [6, 7]\n","BiLSTM_hidden_units = [8, 16, 32, 64, 128]\n","n_layer = [1, 2, 3]\n","n_batch = [2, 4, 8, 16, 32, 64, 128]\n","n_steps = [2]\n","patience_list = [3, 5, 7, 10]\n","mul_uni = ['uni','mul']\n","dropouts = [0.2, 0.3, 0.5]\n","\n","\n","timezone_offset = 7.0  # Bangkok Time (UTC+07:00)\n","tzinfo = timezone(timedelta(hours=timezone_offset))\n","count = 1\n","performance_json = []\n","for g in AHB_list:\n","    for h in patience_list:\n","        for a in n_steps:\n","            for b in dropouts:\n","                for c in BiLSTM_hidden_units:\n","                    for d in n_layer:\n","                        for e in n_batch:\n","                            for f in mul_uni:\n","\n","                                print('PROGRESS: [ ' + str(count) + ' / ' + str(len(dropouts)*len(AHB_list)*len(n_steps)*len(BiLSTM_hidden_units)*len(n_layer)*len(n_batch)*len(patience_list)*len(mul_uni)) + ' ]')\n","                                \n","                                if f == 'uni':\n","                                    if g == 13:\n","                                        raw_seq = list(df_list[g]['Country_Level'])\n","                                        ahb = 'Country_Level'\n","                                    else:    \n","                                        raw_seq = list(df_list[g]['AHB_'+str(g+1)])\n","                                        ahb = 'AHB_'+str(g+1)\n","                                    n_features = 1\n","                                    scaler = get_scaler('minmax')\n","                                    raw_seq_arr = scaler.fit_transform(np.array(raw_seq).reshape(len(raw_seq),-1))\n","                                    X, y = uni_split_sequence(raw_seq_arr, a)\n","                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, train_size=0.9, shuffle=False, stratify=None)\n","                                    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n","                                    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n","                                if f == 'mul':\n","                                    if g == 13:\n","                                        raw_seq = list(df_list[13].drop(df_list[13].columns[:77], axis=1).values)\n","                                        ahb = 'Country_Level'\n","                                        scaler_y = get_scaler('minmax')\n","                                        scaler_y.fit(np.array(df_list[13]['Country_Level']).reshape(len(df_list[13]['Country_Level']),-1))\n","                                    else:    \n","                                        raw_seq = list(df_list[g].drop(['Country_Level'], axis=1).values)\n","                                        ahb = 'AHB_'+str(g+1)\n","                                        scaler_y = get_scaler('minmax')\n","                                        scaler_y.fit(np.array(df_list[g]['AHB_'+str(g+1)]).reshape(len(df_list[13]['AHB_'+str(g+1)]),-1))\n","                                    n_features = len(raw_seq[0])-1\n","                                    scaler = get_scaler('minmax')\n","                                    raw_seq_arr = scaler.fit_transform(np.array(raw_seq))\n","                                    X, y = multi_split_sequences(raw_seq_arr, a)\n","                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, train_size=0.9, shuffle=False, stratify=None)\n","                                    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n","                                    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n","\n","\n","                                callback = EarlyStopping(monitor='loss', patience=h)\n","                                model = Sequential()\n","                                if d == 1:\n","                                    model.add(Bidirectional(LSTM(units=c, return_sequences=False, input_shape=(a,n_features), activation='tanh')))\n","                                    model.add(Dropout(b))\n","                                if d == 2:\n","                                    model.add(Bidirectional(LSTM(units=c, return_sequences=True, input_shape=(a,n_features), activation='tanh')))\n","                                    model.add(Dropout(b))\n","                                    model.add(Bidirectional(LSTM(units=c, return_sequences=False, activation='tanh')))\n","                                    model.add(Dropout(b))\n","                                if d == 3:\n","                                    model.add(Bidirectional(LSTM(units=c, return_sequences=True, input_shape=(a,n_features), activation='tanh')))\n","                                    model.add(Dropout(b))\n","                                    model.add(Bidirectional(LSTM(units=c, return_sequences=True, input_shape=(a,n_features), activation='tanh')))\n","                                    model.add(Dropout(b))\n","                                    model.add(Bidirectional(LSTM(units=c, return_sequences=False, activation='tanh')))\n","                                    model.add(Dropout(b))\n","                                model.add(Dense(units=1, kernel_initializer='he_uniform', activation='linear'))\n","                                model.compile(loss='mse', optimizer='adam')  \n","                                history = model.fit(X_train, y_train, epochs=2000, batch_size=e, verbose=0, callbacks=[callback])\n","                                last_epoch = len(history.history['loss']) \n","\n","                                yhat = model.predict(X_test, verbose=0)\n","                                yhat = yhat.reshape(yhat.shape[0],-1)\n","                                mae = mean_absolute_error(y_test, yhat)\n","                                mse = mean_squared_error(y_test, yhat)\n","                                rmse = mean_squared_error(y_test, yhat, squared = False)\n","                                mape = mean_absolute_percentage_error(y_test, yhat)\n","                                \n","                                if f == 'uni':\n","                                    yhat_inverse = scaler.inverse_transform(yhat)\n","                                    y_test_inverse = scaler.inverse_transform(y_test)\n","                                if f == 'mul':\n","                                    yhat_inverse = scaler_y.inverse_transform(yhat)\n","                                    y_test_inverse = scaler_y.inverse_transform(y_test.reshape(len(y_test),-1))\n","\n","\n","                                mae_inverse = mean_absolute_error(y_test_inverse, yhat_inverse)\n","                                mse_inverse = mean_squared_error(y_test_inverse, yhat_inverse)\n","                                rmse_inverse = mean_squared_error(y_test_inverse, yhat_inverse, squared = False)\n","                                mape_inverse = mean_absolute_percentage_error(y_test_inverse, yhat_inverse)\n","\n","                                performance_json.append({'model': 'BiLSTM', 'AHB_number': ahb, 'Multi_Uni': f, \\\n","                                                         'n_layer': d, 'BiLSTM_hidden_units': c, 'dropouts': b,'n_batch': e,\\\n","                                                         'n_steps': a, 'last_epoch': last_epoch, 'patience_list': h, 'MAE_normalize': float(mae), \\\n","                                                         'MSE_normalize': float(mse), 'RMSE_normalize': float(rmse), 'MAPE_normalize': float(mape), \\\n","                                                         'MAE': float(mae_inverse), 'MSE': float(mse_inverse), 'RMSE': float(rmse_inverse), 'MAPE': float(mape_inverse)})\n","                                \n","\n","                                json_formatted_str = json.dumps(performance_json[-1], indent=2)\n","                                print(json_formatted_str)\n","                                print('Percent CPU Usage: ' + str(psutil.cpu_percent()))\n","                                print('Percent Ram Usage: ' + str(psutil.virtual_memory()[2]))\n","                                print(datetime.now(tzinfo))\n","\n","                                count = count + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsB2cwuT2feQ"},"outputs":[],"source":["name = \"all_performance_BiLSTM_2nsteps_AHB7-8\"\n","json.dump(performance_json, open(name + \".json\",\"w\"))\n","df_json = pd.read_json('/content/' + name + '.json')\n","df_json.to_csv('/content/' + name + '.csv', index=False)\n","#! cp /content/all_performance_BiLSTM_2nsteps_AHB7-8.json /content/drive/MyDrive/Depression\n","! cp /content/all_performance_BiLSTM_2nsteps_AHB7-8.csv /content/drive/MyDrive/Depression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veQ12ze82xhZ"},"outputs":[],"source":["df_json"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"mount_file_id":"12PAUL0tj8hT1A_6XbAt435HpMfJGI8Mn","authorship_tag":"ABX9TyMTPOVZm16XxD4RLwRzdcvr"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}